{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Build a Powerful Hypercube","text":"<p>More analysis, less plumbing. Cube Alchemy automatically transforms your disconnected pandas DataFrames into a unified, multidimensional data model.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome. Please open an issue to discuss major changes first. PRs should include concise descriptions and, where applicable, tests or examples.</p>"},{"location":"examples/","title":"Examples","text":"<p>(Reserved for advanced, end-to-end examples. To be added.)</p>"},{"location":"installation/","title":"Installation","text":"<p>Requires Python 3.8+.</p> <p></p> <pre><code>python -m venv venv\nvenv\\Scripts\\activate\npip install cube-alchemy\n</code></pre>"},{"location":"overview/","title":"Overview","text":"<p>Cube Alchemy transforms your pandas DataFrames into a powerful hypercube, creating a unified semantic layer for multidimensional analysis. This allows you to move from disconnected tables to a coherent analytical model where you can explore data simply and declaratively.</p>"},{"location":"overview/#core-capabilities","title":"Core Capabilities","text":"<ul> <li>Automatic Relationships:  Discovers relationships between your DataFrames by matching shared column names.<ul> <li>Complex Relationship Handling: Handles composite keys and complex relationships transparently.</li> </ul> </li> <li>Multidimensional Analytics: Slice, dice, and aggregate your data across any dimension with consistent, reusable metrics and queries that reduce boilerplate code.</li> <li>Stateful Analysis: Maintain a filtering context across queries to easily compare different scenarios.</li> <li>Interactive &amp; Scalable: Works seamlessly in notebook and data apps (Streamlit/Panel).</li> </ul>"},{"location":"overview/#the-semantic-layer","title":"The Semantic Layer","text":"<p>Map your data into a clear and consistent set of analytical assets to work with your hypercube:</p> <ul> <li>Dimensions: The \"by\" of your analysis\u2014the entities you use to slice and dice data (e.g., <code>Customer</code>, <code>Region</code>, <code>Product</code>).</li> <li>Metrics: The key performance indicators (KPIs) you measure (e.g., <code>Total Revenue</code>, <code>Conversion Rate</code>, <code>Average Order Value</code>).</li> <li>Queries: The questions you ask of your data, combining metrics and dimensions to produce insights (e.g., Revenue by Region over Time).</li> </ul>"},{"location":"overview/#why-it-matters","title":"Why It Matters","text":"<p>Build faster, reliable analytics with a fraction of the effort.</p> <ul> <li>Accelerate Insights: Get into deep analysis in minutes. Relationships are discovered automatically, not manually coded.</li> <li>Simplify Complexity: Replace ad-hoc joins and messy code with clean, declarative queries that are easy to read and maintain.</li> <li>Ensure Consistency: Standardized metrics and a central data model guarantee that everyone gets reliable, consistent results.</li> <li>Integrate Seamlessly: Designed to work with Streamlit and other Python-based frameworks for building interactive data applications.</li> </ul>"},{"location":"streamlit/","title":"Streamlit Integration","text":"<p>(Reserved for a more advanced guide than the README. To be added.)</p>"},{"location":"api/analytics_assets/","title":"Hypercube Analyitics Assets","text":"<p>These methods let you see what is available and how your dimensions, metrics and queries are defined in your hypercube.</p>"},{"location":"api/analytics_assets/#get-dimensions","title":"Get Dimensions","text":"<pre><code>cube.get_dimensions() -&gt; List[str]\n</code></pre> <p>Returns a list of all available dimension columns across all tables.</p> <p>Example:</p> <pre><code># Get all available dimensions\nall_dimensions = cube.get_dimensions()\nprint(f\"Available dimensions: {all_dimensions}\")\n</code></pre>"},{"location":"api/analytics_assets/#get-a-single-dimensions-values","title":"Get a Single Dimension's Values","text":"<p>This accessor now lives under Query Methods as <code>cube.dimension(...)</code>.</p> <pre><code>cube.dimension(dimension: str) -&gt; List[str]\n</code></pre> <p>Returns the distinct values for a given dimension.</p> <p>Example:</p> <pre><code># Get all distinct Product names\nproducts = cube.dimension('Product')\nprint(products[:10])\n</code></pre>"},{"location":"api/analytics_assets/#get-metrics","title":"Get Metrics","text":"<pre><code>cube.get_metrics() -&gt; Dict[str, Any]\n</code></pre> <p>Retrieve all defined metrics in the hypercube. Metrics are one of the core components of the hypercube, representing the calculated values and KPIs used for data analysis. Metrics encapsulate business logic, ensuring consistent calculations across all queries and reports.</p> <p>Returns:</p> <ul> <li>Dictionary of metrics with their details (name, expression, aggregation, and other properties)</li> </ul>"},{"location":"api/analytics_assets/#get-a-single-metric","title":"Get a Single Metric","text":"<pre><code>cube.get_metric(metric: str) -&gt; Dict[str, Any]\n</code></pre> <p>Returns a single metric definition with its details.</p> <p>Example:</p> <pre><code>revenue = cube.get_metric('Revenue')\nprint(revenue)\n</code></pre>"},{"location":"api/analytics_assets/#get-computed-metrics","title":"Get Computed Metrics","text":"<pre><code>cube.get_computed_metrics() -&gt; Dict[str, Any]\n</code></pre> <p>Retrieve all persisted computed metrics.</p> <p>Returns:</p> <ul> <li>Dictionary mapping computed metric names to specs: expression, optional fillna, and referenced columns</li> </ul>"},{"location":"api/analytics_assets/#get-a-single-computed-metric","title":"Get a Single Computed Metric","text":"<pre><code>cube.get_computed_metric(computed_metric: str) -&gt; Dict[str, Any]\n</code></pre> <p>Returns a single computed metric definition.</p> <p>Example:</p> <pre><code>margin_pct = cube.get_computed_metric('Margin %')\nprint(margin_pct)\n</code></pre>"},{"location":"api/analytics_assets/#get-queries","title":"Get Queries","text":"<pre><code>cube.get_queries() -&gt; Dict[str, Any]\n</code></pre> <p>Returns:</p> <ul> <li>Dictionary of queries with their dimensions, metrics, and display options</li> </ul>"},{"location":"api/analytics_assets/#get-a-single-query","title":"Get a Single Query","text":"<pre><code>cube.get_query(query: str) -&gt; Dict[str, Any]\n</code></pre> <p>Returns the definition for a single query (dimensions, metrics, computed_metrics, and options like having).</p> <p>Example:</p> <pre><code>q = cube.get_query('Sales by Region')\nprint(q)\n</code></pre>"},{"location":"api/filter_methods/","title":"Filter Methods","text":""},{"location":"api/filter_methods/#apply-filter","title":"Apply Filter","text":"<pre><code>filter(\n  criteria: Dict[str, List[Any]], \n  context_state_name: str = 'Default'\n) -&gt; bool\n</code></pre> <p>Apply filters to focus analysis on specific data slices.</p> <p>Parameters:</p> <ul> <li><code>criteria</code>: Dictionary mapping dimension names to lists of values to keep</li> <li><code>context_state_name</code>: Which context state to filter</li> </ul> <p>Returns:</p> <ul> <li>Boolean indicating success</li> </ul> <p>Example:</p> <pre><code># Filter to specific regions\ncube.filter({'region': ['North', 'South']})\n\n# Filter to specific products within those regions\ncube.filter({'product': ['Electronics', 'Home']})\n\n# Filter in a different context state\ncube.filter({'region': ['West']}, context_state_name='State1')\n</code></pre>"},{"location":"api/filter_methods/#remove-filter","title":"Remove Filter","text":"<pre><code>remove_filter(\n  dimensions: List[str],\n  context_state_name: str = 'Default'\n) -&gt; bool\n</code></pre> <p>Remove filters from specified dimensions.</p> <p>Parameters:</p> <ul> <li><code>dimensions</code>: List of dimension names to remove filters from</li> <li><code>context_state_name</code>: Which context state to modify</li> </ul> <p>Returns:</p> <ul> <li>Boolean indicating success</li> </ul> <p>Example:</p> <pre><code># Remove region filter\ncube.remove_filter(['region'])\n</code></pre>"},{"location":"api/filter_methods/#reset-filters","title":"Reset Filters","text":"<pre><code>reset_filters(\n  direction: str = 'backward',\n  context_state_name: str = 'Default'\n) -&gt; bool\n</code></pre> <p>Reset filters using undo/redo functionality or clear all filters.</p> <p>Parameters:</p> <ul> <li><code>direction</code>: 'backward' (undo), 'forward' (redo), or 'all' (clear all filters)</li> <li><code>context_state_name</code>: Which context state to modify</li> </ul> <p>Returns:</p> <ul> <li>Boolean indicating success</li> </ul> <p>Example:</p> <pre><code># Undo last filter operation\ncube.reset_filters('backward')\n\n# Redo previously undone filter operation\ncube.reset_filters('forward')\n\n# Clear all filters\ncube.reset_filters('all')\n</code></pre>"},{"location":"api/filter_methods/#get-filters","title":"Get Filters","text":"<pre><code>get_filters(\n  off_set: int = 0,\n  context_state_name: str = 'Default'\n) -&gt; Dict[str, List[Any]]\n</code></pre> <p>Get currently active filters.</p> <p>Parameters:</p> <ul> <li><code>off_set</code>: How many steps back in filter history to look</li> <li><code>context_state_name</code>: Which context state to check</li> </ul> <p>Returns:</p> <ul> <li>Dictionary of active filters</li> </ul> <p>Example:</p> <pre><code># Get current filters\ncurrent_filters = cube.get_filters()\nprint(f\"Current filters: {current_filters}\")\n</code></pre>"},{"location":"api/hypercube/","title":"Hypercube API","text":"<p>The Hypercube class is the central component of Cube Alchemy, providing methods for creating, querying, and analyzing multidimensional data.</p>"},{"location":"api/hypercube/#initialization","title":"Initialization","text":"<p>The Hypercube can be initialized in two ways:</p> <pre><code># Option 1: Initialize with data\nHypercube(\n  tables: Dict[str, pd.DataFrame] = None,\n  rename_original_shared_columns: bool = True,\n  apply_composite: bool = True,\n  validate: bool = True,\n  to_be_stored: bool = False\n)\n\n# Option 2: Initialize empty and load data later\nHypercube()\nload_data(\n  tables: Dict[str, pd.DataFrame],\n  rename_original_shared_columns: bool = True,\n  apply_composite: bool = True,\n  validate: bool = True,\n  to_be_stored: bool = False,\n  reset_all: bool = False\n)\n</code></pre> <p>The <code>load_data()</code> method can also be used to reload or update data in an existing hypercube.</p> <p>Parameters:</p> <ul> <li><code>tables</code>: Dictionary mapping table names to pandas DataFrames</li> <li><code>rename_original_shared_columns</code>: Controls what happens to shared columns in source tables.  </li> <li>True (default): keep them, renamed as <code>&lt;column&gt; (&lt;table_name&gt;)</code>. Enables per\u2011table counts/aggregations.  </li> <li>False: drop them from source tables (values remain in link tables). Saves time and memory if per\u2011table analysis isn\u2019t needed.  </li> <li><code>apply_composite</code>: Whether to automatically create composite keys for multi-column relationships</li> <li><code>validate</code>: Whether to validate schema and build trajectory cache during initialization</li> <li><code>to_be_stored</code>: Set to True if the hypercube will be serialized/stored (skips Default context state creation)</li> <li><code>reset_all</code> (only load_data method): Whether to reset metrics and queries definitions, as well as registered functions when reloading data</li> </ul> <p>Examples:</p> <pre><code>import pandas as pd\nfrom cube_alchemy import Hypercube\n\n# Option 1: Initialize with data (keep renamed shared columns)\ncube1 = Hypercube({\n    'Product': products_df,\n    'Customer': customers_df,\n    'Sales': sales_df\n}, rename_original_shared_columns=True)\n\n# Option 2: Initialize empty first, then load data\ncube2 = Hypercube()\ncube2.load_data({\n    'Product': products_df,\n    'Customer': customers_df,\n    'Sales': sales_df\n}, rename_original_shared_columns=False)\n\n# Reload data in an existing hypercube (e.g., when data is updated)\ncube1.load_data({\n    'Product': updated_products_df,\n    'Customer': updated_customers_df,\n    'Sales': updated_sales_df\n})\n\n# Reset metrics and queries when loading new data schema\ncube2.load_data(new_data, reset_all=True)\n</code></pre>"},{"location":"api/hypercube/#core-methods","title":"Core Methods","text":"<p>visualize_graph</p> <pre><code>visualize_graph(layout_type: str = 'spring', w: int = 12, h: int = 8, full_column_names: bool = True) -&gt; None\n</code></pre> <p>Visualize the relationships between tables as a network graph.</p> <p>Parameters:</p> <ul> <li> <p><code>layout_type</code>: Algorithm for graph layout. Options include:</p> <ul> <li><code>'spring'</code>(default)</li> <li><code>'circular'</code></li> <li><code>'shell'</code></li> <li><code>'random'</code></li> <li><code>'kamada_kawai'</code></li> <li><code>'spectral'</code></li> <li><code>'planar'</code></li> <li><code>'spiral'</code></li> </ul> </li> <li> <p><code>w</code>: Width of the plot</p> </li> <li> <p><code>h</code>: Height of the plot</p> </li> <li> <p><code>full_column_names</code>: Whether to show renamed columns with table reference (e.g., <code>column &lt;table_name&gt;</code>) or just the original column names</p> </li> </ul> <p>Example:</p> <p><pre><code># Visualize the data model relationships\ncube.visualize_graph()\n\n# Hide renamed column format for cleaner display\ncube.visualize_graph('spring', w=20, h=12, full_column_names=False)\n</code></pre> Note: If the displayed graph doesn't look so good try a couple of times or adjust the size.</p> <p>set_context_state</p> <pre><code>set_context_state(context_state_name: str, base_context_state_name: str = 'Unfiltered') -&gt; bool\n</code></pre> <p>Create a new context state for independent filtering environments.</p> <p>Parameters:</p> <ul> <li><code>context_state_name</code>: Name for the new context state</li> <li><code>base_context_state_name</code>: Name of the base context state, the new context state will be a copy of this one.</li> </ul> <p>Returns:</p> <ul> <li>Boolean indicating success</li> </ul> <p>Example:</p> <pre><code># Create a new context state\ncube.set_context_state('Marketing Analysis') # Creates a copy from the unfiltered context state --&gt; self.context_states['Marketing Analysis'] = self.context_states['Unfiltered']\n\n# Apply filters specific to this context\ncube.filter({'channel': ['Email', 'Social']}, context_state_name='Marketing Analysis')\n</code></pre>"},{"location":"api/hypercube/#shared-columns-counts-and-distincts","title":"Shared columns: counts and distincts","text":"<p>When multiple tables share a column (for example, <code>customer_id</code>), Cube Alchemy builds a link table containing the distinct values of that column across all participating tables. This has two practical implications:</p> <ul> <li>Counting on the shared column name (e.g., <code>customer_id</code>) uses the link table and therefore reflects distinct values in the current filtered context across all tables that share it.</li> <li>Counting on a per-table renamed column (e.g., <code>customer_id &lt;orders&gt;</code> or <code>customer_id &lt;customers&gt;</code>) uses that table\u2019s own column values. The result can differ from the shared-column count because it\u2019s scoped to that single table's values and is not the cross-table distinct set.</li> </ul> <p>Example idea:</p> <ul> <li>Count distinct <code>customer_id</code> (shared) \u2192 distinct customers across all linked tables.</li> <li>Count distinct <code>customer_id &lt;orders&gt;</code> \u2192 distinct customers present in the Orders table specifically.</li> </ul> <p>Choose the one that matches your analytical intent: cross-table distincts via the shared column, or table-specific distincts via the renamed columns. Note: per-table renamed columns are available only when <code>rename_original_shared_columns=True</code>; set it to False to drop them and reduce memory/processing if you don\u2019t need that analysis.</p>"},{"location":"api/metric_methods/","title":"Metric Methods","text":""},{"location":"api/metric_methods/#define-metric","title":"Define Metric","text":"<pre><code>define_metric(\n    name: Optional[str] = None,\n    expression: Optional[str] = None,\n    aggregation: Optional[Union[str, Callable[[Any], Any]]] = None,\n    metric_filters: Optional[Dict[str, Any]] = None,\n    row_condition_expression: Optional[str] = None,\n    context_state_name: str = 'Default',\n    ignore_dimensions: Optional[Union[bool, List[str]]] = False,\n    fillna: Optional[any] = None\n)\n</code></pre> <p>Defines a metric and stores it in the cube object for later use in queries.</p> <p>Parameters:</p> <ul> <li><code>name</code>: Label for the metric (used in query results)</li> <li><code>expression</code>: Calculation formula using [column] references and @custom_functions</li> <li><code>aggregation</code>: How to combine values - pandas aggregation string ('sum', 'mean', 'count') or custom callable</li> <li><code>metric_filters</code>: Filters applied only when evaluating this specific metric</li> <li><code>row_condition_expression</code>: Filter expression applied to rows before calculating the metric</li> <li><code>context_state_name</code>: Which context state this metric operates in</li> <li><code>ignore_dimensions</code>: Control how dimensions affect aggregation - <code>True</code> to ignore all dimensions (grand total), a list of dimension names to ignore specific dimensions, or <code>False</code> (default) for normal dimensional aggregation</li> <li><code>fillna</code>: Value to use for replacing Null values on the metric expression and row_condition_expression columns before aggregation. Note: This parameter applies the same value to all columns. For column-specific NA handling, use <code>@pd.fillna()</code> or <code>@np.nan_to_num()</code> functions directly in the expression.</li> </ul> <p>Example:</p> <pre><code># Define a revenue metric\ncube.define_metric(\n    name='Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum'\n)\n\n# Sum sales only from Australia\n# Note: This differs from using metric_filters={'region': ['Australia']} in how the filtering is applied:\n# - row_condition_expression: Fetches all the rows, then applies pandas .query() with backtick syntax\n# - metric_filters: Applied at the context state level before metric calculation\n# The row_condition_expression fetches all the rows for the column which can result in different aggregation values depending on your data relationships.\ncube.define_metric(\n    name='Australia Sales',\n    expression='[sales_amount]',\n    aggregation='sum',\n    row_condition_expression='[region] == \"Australia\"'\n)\n\n# Define a metric that ignores all dimensions (calculates grand total)\ncube.define_metric(\n    name='Total Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    ignore_dimensions=True  # Ignore all dimensions when aggregating\n)\n\n# Define a metric that ignores specific dimensions (partial total)\ncube.define_metric(\n    name='Revenue by Country', \n    expression='[qty] * [price]',\n    aggregation='sum',\n    ignore_dimensions=['city', 'product_category']  # Ignore these dimensions, aggregate only by remaining dimensions\n)\n\n# Advanced NA handling - Using the simple fillna parameter (fills all columns with same value)\ncube.define_metric(\n    name='Revenue with NA Handling',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    fillna=0  # Fills both qty and price with 0\n)\n\n# Advanced NA handling - Using @functions for column-specific handling\ncube.define_metric(\n    name='Revenue with Column-Specific NA Handling',\n    expression='@pd.Series([qty]).fillna(1) * @pd.Series([price]).fillna(0)',\n    aggregation='sum'  # Fills [qty] with 1 and [price] with 0\n)\n</code></pre> <p>Syntax Rules</p> <ul> <li> <p>Column References: Columns in metric expressions must be enclosed in square brackets: <code>[qty]</code>, <code>[price]</code>, <code>[cost]</code>, etc.</p> </li> <li> <p>Aggregation Methods: The <code>aggregation</code> parameter accepts:</p> </li> <li> <p>Pandas group by strings: <code>'sum'</code>, <code>'mean'</code>, <code>'count'</code>, <code>'min'</code>, <code>'max'</code>, etc.</p> </li> <li> <p>Custom callable functions: <code>lambda x: x.quantile(0.95)</code> or any function that accepts a pandas Series</p> </li> </ul>"},{"location":"api/metric_methods/#define-computed-metric","title":"Define Computed Metric","text":"<pre><code>define_computed_metric(\n    name: str,\n    expression: str,\n    fillna: Optional[Any] = None\n) -&gt; None\n</code></pre> <p>Defines a post-aggregation computed metric that operates on already aggregated metrics and dimensions.</p> <p>Parameters:</p> <ul> <li><code>name</code>: Unique label for the metric (used in query results)</li> <li><code>expression</code>: Calculation formula using [MetricName] references and dimension column references</li> <li><code>fillna</code>: Optional value to replace NaN computed metrics expression columns before calculation.</li> </ul> <p>Example:</p> <pre><code># Define a profit margin percentage computed metric\ncube.define_computed_metric(\n    name='Margin %',\n    expression='([Revenue] - [Cost]) / [Revenue] * 100',\n    fillna=0\n)\n</code></pre> <p>Using in queries:</p> <p>You must define computed metrics first, then reference them by name in queries:</p> <pre><code>cube.define_query(\n    query_name='sales_margin_named',\n    dimensions={'product'},\n    metrics=['Revenue', 'Cost'],\n    computed_metrics=['Margin %'],\n)\n</code></pre>"},{"location":"api/metric_methods/#register-function","title":"Register Function","text":"<pre><code>register_function(**kwargs)\n</code></pre> <p>Register custom functions to use in metric expressions with @function_name syntax.</p> <p>Parameters:</p> <ul> <li><code>**kwargs</code>: Keyword arguments where each key is the name to use in expressions and each value is the function</li> </ul> <p>Example:</p> <pre><code>import numpy as np\n\n# Define a custom function\ndef safe_division(numerator, denominator, default=0.0):\n    \"\"\"Safely divide two arrays, handling division by zero\"\"\"\n    result = numerator / denominator\n    return result.replace([np.inf, -np.inf], np.nan).fillna(default)\n\n# Register the function with your hypercube\ncube.register_function(safe_division=safe_division)\n\n# Use in metric definition\ncube.define_metric(\n    name='Profit Margin %',\n    expression='@safe_division([revenue] - [cost], [revenue]) * 100',\n    aggregation='mean'\n)\n</code></pre>"},{"location":"api/query_methods/","title":"Query Methods","text":""},{"location":"api/query_methods/#define-query","title":"Define Query","text":"<pre><code>define_query(\n    name: str,\n    dimensions: set[str] = {},\n    metrics: List[str] = [],\n    computed_metrics: Optional[List[str]] = None,\n    having: Optional[str] = None,\n    drop_null_dimensions: bool = False,\n    drop_null_metric_results: bool = False\n)\n</code></pre> <p>Defines a named query with dimensions and metrics for later execution.</p> <p>Parameters:</p> <ul> <li><code>name</code>: A unique name for the query</li> <li><code>dimensions</code>: Set of dimension column names to include in the query</li> <li><code>metrics</code>: List of metric names (as defined with define_metric)</li> <li><code>computed_metrics</code>: List of names referencing computed metrics defined via <code>define_computed_metric()</code></li> <li><code>having</code>: SQL HAVING-like expression to filter rows based on aggregated results</li> <li><code>drop_null_dimensions</code>: Whether to exclude rows where dimension values are missing</li> <li><code>drop_null_metric_results</code>: Whether to exclude rows where metric calculations result in null</li> </ul> <p>Example:</p> <pre><code># Define metrics\ncube.define_metric(name='Revenue', expression='[qty] * [price]', aggregation='sum')\ncube.define_metric(name='Units', expression='[qty]', aggregation='sum')\n\ncube.define_computed_metric(name = 'Avg Revenue per Unit', expression = '[Revenue] / [Units]')\n\n# Define a query using those metrics\ncube.define_query(\n    query_name=\"sales analysis\",\n    dimensions={'region', 'category', 'promo_type'},\n    metrics=['Revenue'],\n    computed_metrics=['Revenue per Unit'],\n    having='[Revenue per Unit] &gt; 10'\n)\n</code></pre>"},{"location":"api/query_methods/#execute-query","title":"Execute Query","text":"<pre><code>query(query_name: str, return_internal_metrics: bool = False) -&gt; pd.DataFrame\n</code></pre> <p>Executes a previously defined query by name.</p> <p>Parameters:</p> <ul> <li><code>query_name</code>: Name of a previously defined query</li> <li><code>return_internal_metrics</code>: The query might need metrics and/or computed metrics which are internally used for calculation of computed metrics, having and/or sorting. If this parameter is set to True they will also be retrieved. Can be useful for debugging or validation.</li> </ul> <p>Returns:</p> <ul> <li>Pandas DataFrame with the query results</li> </ul> <p>Example:</p> <pre><code># Define a query\ncube.define_query(\n    query_name=\"sales_analysis\",\n    dimensions=set(['region', 'category']),\n    metrics=['Revenue', 'Units']\n)\n\n# Execute the query\nresult = cube.query(\"sales_analysis\")\n\n# Apply a filter and run the same query again\ncube.filter({'region': ['North']})\nfiltered_result = cube.query(\"sales_analysis\")\n</code></pre>"},{"location":"api/query_methods/#dimensions-only","title":"Dimensions Only","text":"<pre><code>dimensions(\n  columns_to_fetch: List[str],\n  retrieve_keys: bool = False, # Internal use, these are autogenerated autonumbered keys each of shared column names have.\n  context_state_name: str = 'Default',\n  query_filters: Optional[Dict[str, Any]] = None\n) -&gt; pd.DataFrame\n</code></pre> <p>Fetch unique values for specified dimension columns.</p> <p>Parameters:</p> <ul> <li><code>columns_to_fetch</code>: List of dimension column names to retrieve</li> <li><code>retrieve_keys</code>: Whether to include link table keys in the result</li> <li><code>context_state_name</code>: Which context state to use</li> <li><code>query_filters</code>: Additional filters to apply just for this query</li> </ul> <p>Returns:</p> <ul> <li>Pandas DataFrame with unique combinations of the requested dimensions</li> </ul> <p>Example:</p> <pre><code># Get unique region/category combinations\nresult = cube.dimensions(['region', 'category'])\n\n# Get unique region/category combinations from a specific context state\nresult = cube.dimensions(['region', 'category'], context_state_name='State1')\n</code></pre>"},{"location":"api/query_methods/#single-dimension-values","title":"Single Dimension Values","text":"<p>Fetch distinct values for a single dimension.</p> <pre><code>dimension(dimension: str) -&gt; pd.DataFrame\n</code></pre> <p>Example:</p> <pre><code>products = cube.dimension('Product')\n</code></pre>"},{"location":"concepts/context-state/","title":"Context State","text":"<p>You can think of it as a separate filtering environment.</p> <p>There are two main context states:</p> <ul> <li> <p>'Unfiltered': This is created at the hypercube initialization. It has all the relationships that exists accross all the tables that conform the DAG when there is no filter applied. It is a special internal use state, and is not allowed to filter on it.</p> </li> <li> <p>'Default': When initializing the hypercube on memory, we take a copy of the unfiltered state. Then by default the filters and queries will use this context state, but you can change that..</p> </li> </ul> <p>More states (?)</p> <p>You can also create more context states, as many as you want. They allow you to create multiple independent filtering contexts within the same app so you can compare, isolate, simulate different scenarios side by side or use to create more advanced filtering scenarios.</p> <p>Creating new context states:</p> <pre><code># Create a new filtering environment\ncube.set_context_state('New Analysis')\n\n# Apply filters specific to this context\ncube.filter(\n    {'date': ['2024-10-01', '2024-11-01', '2024-12-01']}, \n    context_state_name='New Analysis'\n)\n\n# Define metrics using the specific context state\ncube.define_metric(\n    name='New Analysis Revenue',\n    expression='[qty] * [price]', \n    aggregation='sum',\n    context_state_name='New Analysis'  # This metric uses the New Analysis context\n)\n\n# Define regular metrics (using Default context)\ncube.define_metric(\n    name='Regular Revenue',\n    expression='[qty] * [price]', \n    aggregation='sum'\n    # No context_state_name means it uses 'Default'\n)\n\n# Define a query that will use both metrics\ncube.define_query(\n    name=\"revenue_comparison\",\n    dimensions={'region'},\n    metrics=['Regular Revenue', 'New Analysis Revenue']\n    # Queries don't have context_state_name parameter\n)\n\n# Execute the query - it will show revenue from both context states\ncomparison_results = cube.query(\"revenue_comparison\")\n</code></pre> <p>Bear in mind that when creating a new context state, a copy of the 'Unfiltered' state is made and stored on memory.</p> <p>Note</p> <p>In a more realistic workflow, when suporting multiple users/sessions with the same data model, you might not want to initialize and build the relationships (deploy the keys) accross the entire DAG in every session.</p> <p>One way of dealing with this is by storing the already initialized hypercube (eg. using pickle) and then retrieving it later on when a user needs it. In this case it is better if you do not store the <code>Default</code> on disk and instead you set it once the hypercube is loaded. </p> <p>Make sure if this is the case that you perform cube.set_context_state('Default') right after loading it, so the default state will be present and the hypercube will work properly.</p>"},{"location":"concepts/dimensions/","title":"Dimensions","text":"<p>Dimensions are the categorical columns from your DataFrames that you use to slice and group your data. Think of them as the \"by what\" in your analysis\u2014region, product category, time period, customer segment, and so on.</p> <p>In Cube Alchemy, any column from any table in your hypercube can serve as a dimension.</p> <p>How dimensions work:</p> <ul> <li>Available everywhere: Once your DataFrames are connected in the hypercube, dimensions from any table can be used in any query</li> <li>Auto-joining: When you query dimensions from different tables, the hypercube automatically traverses the relationships to bring them together</li> <li>Multi-hop: You can combine dimensions that are several \"hops\" apart in your data model\u2014like querying by both customer region and product category even if they're in completely separate tables</li> </ul> <p>Getting dimension values:</p> <pre><code># Get combinations of region and category\ncube.dimensions(['region', 'category'])\n\n# Or you can use the query method, but you first need to define the query without metrics\ncube.define_query(\n    name=\"dimension_combinations\",\n    dimensions=set(['region', 'category'])\n)\ncube.query('dimension_combinations')\n</code></pre>"},{"location":"concepts/filters/","title":"Understanding Filters","text":"<p>Filters in Cube Alchemy let you focus your analysis on specific slices of data without modifying your underlying queries.</p>"},{"location":"concepts/filters/#how-filtering-works","title":"How Filtering Works","text":"<p>When you apply a filter to a context state, all queries and metrics operate only on the filtered data until you change or remove the filter:</p> <ol> <li>Apply a filter to select specific data values</li> <li>Execute queries to analyze just the filtered data</li> <li>Modify or remove filters when you want to change focus</li> </ol> <pre><code># Define metrics first\ncube.define_metric(name='Revenue', expression='[qty] * [price]', aggregation='sum')\n\n# Now all queries only see North and West data\ncube.define_query(\n    name=\"sales\",\n    dimensions={'category'},\n    metrics=['Revenue']\n)\n\n# Focus on specific regions\ncube.filter({'region': ['North', 'West']})\n\ncube.query(\"sales\")\n</code></pre> <p>The beauty of Cube Alchemy filtering is that it works seamlessly across all your connected tables. When you filter by customer region, it automatically affects sales data, product data, and anything else connected through your data model relationships.</p>"},{"location":"concepts/filters/#basic-filtering-operations","title":"Basic Filtering Operations","text":""},{"location":"concepts/filters/#apply-filters","title":"Apply Filters","text":"<p>Filters are defined as dictionaries where keys are dimension names and values are lists of allowed values:</p> <pre><code># Single dimension, multiple values\ncube.filter({'region': ['North', 'West', 'South']})\n\n# Multiple dimensions at once\ncube.filter({\n    'region': ['North', 'West'], \n    'product_category': ['Electronics'],\n    'customer_segment': ['Enterprise', 'SMB']\n})\n</code></pre>"},{"location":"concepts/filters/#remove-specific-filters","title":"Remove Specific Filters","text":"<p>Remove filters from specific dimensions while keeping others intact:</p> <pre><code># Remove filter on region only (keep other filters)\ncube.remove_filter(['region'])\n\n# Remove multiple filters at once\ncube.remove_filter(['region', 'product_category'])\n</code></pre>"},{"location":"concepts/filters/#reset-all-filters","title":"Reset All Filters","text":"<p>Clear all filters to return to the full dataset:</p> <pre><code># Remove all filters (back to unfiltered data)\ncube.reset_filters(direction='all')\n</code></pre>"},{"location":"concepts/filters/#view-current-filters","title":"View Current Filters","text":"<p>Check which filters are currently active:</p> <pre><code># Get dictionary of active filters\ncurrent_filters = cube.get_filters()\nprint(current_filters)\n# Output: {'region': ['North', 'West'], 'category': ['Electronics']}\n</code></pre>"},{"location":"concepts/filters/#filter-history-and-undoredo","title":"Filter History and Undo/Redo","text":"<p>Cube Alchemy maintains your filtering history, allowing you to navigate through previous filter states:</p> <pre><code># Undo last filter operation (step backward)\ncube.reset_filters(direction='backward')\n\n# Redo previously undone filter (step forward)\ncube.reset_filters(direction='forward')\n\n# Clear all filters and history\ncube.reset_filters(direction='all')\n\n# See which dimensions currently have active filters\nfiltered_dims = cube.get_filtered_dimensions()\n</code></pre>"},{"location":"concepts/filters/#advanced-filtering-techniques","title":"Advanced Filtering Techniques","text":""},{"location":"concepts/filters/#per-metric-filters","title":"Per-Metric Filters","text":"<p>While filters apply globally to your hypercube's context state, individual metrics can have their own additional filters:</p> <pre><code># Global filter: only Electronics category\ncube.filter({'category': ['Electronics']})\n\n# Define metric with additional filter: only Premium-tier electronics\ncube.define_metric(\n    name='Premium Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    metric_filters={'price_tier': ['Premium']}  # Additional filter just for this metric\n)\n</code></pre>"},{"location":"concepts/filters/#unfiltered-and-default-context-states","title":"'Unfiltered' and 'Default' Context States","text":"<p>Cube Alchemy maintains two special context states called 'Unfiltered' and 'Default'.</p> <pre><code>- 'Unfiltered' always contains your full dataset in the context and cannot be updated.\n\n- 'Default' is used as default (as you might have guessed).\n</code></pre> <p>And 'Unfiltered' can also be used on the metric context state: </p> <pre><code># Define standard revenue metric in the default (filtered) context\ncube.define_metric(\n    name='Filtered Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum'\n    #, context_state_name='Default'\n)\n\n# Apply some filters to the default context\ncube.filter({'region': ['North', 'South']}) # Applies on 'Default'\n\n# Define a metric using the special Unfiltered context\ncube.define_metric(\n    name='Total Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    context_state_name='Unfiltered'  # Always uses the full dataset\n)\n\n# Define a query that compares filtered and unfiltered metrics\ncube.define_query(\n    name=\"revenue_comparison\",\n    dimensions={'product_category'},\n    metrics=['Filtered Revenue', 'Total Revenue']\n)\n\n# The result will show revenue for North and South regions alongside\n# the total revenue across all regions for each product category\nresult = cube.query(\"revenue_comparison\")\n</code></pre>"},{"location":"concepts/hypercube/","title":"Understanding the Hypercube","text":"<p>A hypercube is a powerful data structure for organizing and analyzing data from multiple perspectives. Think of a simple cube with three dimensions: length, width, and height. A hypercube extends this concept to an unlimited number of dimensions, allowing you to model complex business scenarios.</p> <p>Each axis of the hypercube represents a key dimension of your data, such as:</p> <ul> <li> <p>Time (e.g., year, month, day)</p> </li> <li> <p>Geography (e.g., region, country, city)</p> </li> <li> <p>Product (e.g., category, brand, item)</p> </li> <li> <p>Customer (e.g., segment, demographics)</p> </li> </ul> <p>The hypercube enables you to calculate metrics (like sales, revenue, or user counts) across any combination of these dimensions, ensuring your calculations are always consistent and reusable. For example, you can analyze:</p> <ul> <li>Revenue by Month</li> <li>Active users by Month and Product Category</li> <li>Margin by Month, Product Category, and Region</li> </ul>"},{"location":"concepts/hypercube/#how-cube-alchemy-builds-the-hypercube","title":"How Cube Alchemy Builds the Hypercube","text":"<p>In Cube Alchemy, the hypercube is not limited to a single table. It combines multiple DataFrames into a unified, logical model that you can query seamlessly. You don't need to write the complex joins to connect them; the hypercube handles the relationships for you.</p> <p>Key Concepts:</p> <ul> <li> <p>Data Model as a Graph: The underlying structure is a Directed Acyclic Graph (DAG).</p> <ul> <li>Nodes: Your DataFrames.</li> <li>Edges: The shared columns that link them.</li> </ul> </li> <li> <p>Effortless Queries: When you request a metric across certain dimensions, the hypercube automatically traverses these connections (even across multiple \"hops\") to gather the necessary data.</p> </li> <li> <p>Consistency: Define your metrics and dimensions once, and they can be reliably used across the entire data model.</p> </li> </ul> <p>Important Note: Avoid Circular Dependencies</p> <p>For the hypercube to function correctly, your data model must be a Directed Acyclic Graph (DAG). This means you must avoid cyclic relationships (or circular references). Such cycles break the logic of data traversal and can lead to incorrect or infinite aggregations.</p>"},{"location":"concepts/metrics/","title":"Understanding Metrics","text":"<p>A metric is a calculated measure that aggregates data in some meaningful way. Think revenue, profit margin, average order size, customer count\u2014basically any aggregated value that helps you analyze your data.</p> <p>In Cube Alchemy, metrics are defined once and stored within the cube object for later use:</p> <ol> <li>You define a metric using <code>cube.define_metric()</code> providing at least a name, expression, and aggregation method</li> <li>The metric is stored in the cube object</li> <li>Later, you reference metrics by name when defining queries</li> </ol>"},{"location":"concepts/metrics/#building-a-metric","title":"Building a Metric","text":"<p>Every metric needs three essential components:</p> <ol> <li>Name: A clear, descriptive label for the metric (e.g., 'Revenue', 'Customer Count')</li> <li>Expression: The calculation formula, using dimension references inside square brackets (e.g., <code>[qty] * [price]</code>)</li> <li>Aggregation: How to combine values\u2014standard methods like <code>sum</code>, <code>mean</code>, <code>count</code>, or custom functions</li> </ol> <pre><code># Step 1: Define your metrics\ncube.define_metric(\n    name='Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum'\n)\n\ncube.define_metric(\n    name='Average Order Value', \n    expression='[price]',\n    aggregation='mean'\n)\n\ncube.define_metric(\n    name='Number of Orders',\n    expression='[order_id]',\n    aggregation=lambda x: x.nunique()\n)\n\n# Step 2: Define a query that uses these metrics\ncube.define_query(\n    name=\"sales_performance\",\n    dimensions={'region', 'product_category'},\n    metrics=['Revenue', 'Average Order Value', 'Number of Orders']\n)\n\n# Step 3: Execute the query by referencing its name\nresult = cube.query(\"sales_performance\")\n\n# The metrics are calculated and returned as columns in the result DataFrame\n</code></pre>"},{"location":"concepts/metrics/#syntax-rules","title":"Syntax Rules","text":"<ul> <li> <p>Column References: Columns in metric expressions must be enclosed in square brackets: <code>[qty]</code>, <code>[price]</code>, <code>[cost]</code>, etc.</p> </li> <li> <p>Aggregation Methods: The <code>aggregation</code> parameter accepts:</p> <ul> <li> <p>Pandas group by strings: <code>'sum'</code>, <code>'mean'</code>, <code>'count'</code>, <code>'min'</code>, <code>'max'</code>, etc.</p> </li> <li> <p>Custom callable functions: <code>lambda x: x.quantile(0.95)</code> or any function that accepts a pandas Series</p> </li> </ul> </li> </ul>"},{"location":"concepts/metrics/#computed-metrics","title":"Computed Metrics","text":"<p>Computed metrics are calculated after aggregation has already occurred. While regular metrics aggregate over dimensions, computed metrics work with already aggregated results, letting you create ratios, percentages, and other derivative calculations.</p> <pre><code># First define the metrics needed for the computation\ncube.define_metric(\n    name='Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum'\n)\n\ncube.define_metric(\n    name='Cost',\n    expression='[qty] * [unit_cost]',\n    aggregation='sum'\n)\n\n# Then define a computed metric that uses them\ncube.define_computed_metric(\n    name='Profit Margin %',\n    expression='([Revenue] - [Cost]) / [Revenue] * 100'\n)\n\n# Use both regular and computed metrics in queries\ncube.define_query(\n    name=\"profitability_analysis\",\n    dimensions={'product_category', 'region'},\n    computed_metrics=['Profit Margin %']\n)\n\n# Execute the query\nresult = cube.query(\"profitability_analysis\")\n</code></pre> <p>The workflow is:</p> <ol> <li> <p>Define regular metrics that perform aggregation</p> </li> <li> <p>Define computed metrics that reference those aggregated metrics</p> </li> <li> <p>Include them in your queries (computed metrics are passed separately)</p> </li> </ol>"},{"location":"concepts/metrics/#advanced-features","title":"Advanced Features","text":"<p>For more sophisticated analysis, metrics support several powerful options:</p> <ul> <li>Different context states: Calculate metrics in different filtering environments</li> <li>Metric filters: Apply specific filters only for a particular metric</li> <li>Row conditions: Pre-filter rows before calculating the metric</li> <li>Ignore Dimensions: Control dimensional aggregation behavior</li> <li>Custom functions: Use your own Python functions for complex logic</li> </ul> <p>Each of these options allows you to create highly specialized metrics that can answer specific more sophisticated questions.</p> <pre><code># Only count high-value orders\ncube.define_metric(\n    name='High Value Orders',\n    expression='[order_id]',\n    aggregation='count',\n    row_condition_expression='[price] &gt; 100'\n)\n\n# Revenue only from specific regions (metric-level filter)\ncube.define_metric(\n    name='Regional Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    metric_filters={'region': ['North', 'West']}\n)\n\n# Create a new Context State\ncube.set_context_state('My New Context')\n\n# Apply different filters to it\ncube.filter(\n    {'date': ['2024-10-01', '2024-11-01', '2024-12-01']},\n    context_state_name='My New Context'\n)\n\n# Define a metric using the new context\ncube.define_metric(\n    name='High Value Orders',\n    expression='[order_id]',\n    aggregation='count',\n    context_state_name='My New Context'\n)\n\n# Ignore_dimensions from aggregation\ncube.define_metric(\n    name='Total Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    ignore_dimensions=True  # Ignores all dimensions, returns same value for all rows\n)\n\n# Ignore specific dimensions from aggregation\ncube.define_metric(\n    name='Country Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    ignore_dimensions=['city', 'product', 'date']  # Ignore these dimensions when aggregating\n)\n\n# Define a query with these metrics\ncube.define_query(\n    name=\"advanced_analysis\",\n    dimensions=set(my_query_dimensions),\n    metrics=['High Value Orders', 'Regional Revenue', 'High Value Orders']\n)\n\n# Execute the query\nresult = cube.query(\"advanced_analysis\")\n</code></pre>"},{"location":"concepts/metrics/#custom-functions","title":"Custom Functions","text":"<p>When your analysis requires logic that goes beyond basic arithmetic, you can register and use custom Python functions:</p> <ol> <li>Define a Python function that performs your specialized calculation</li> <li>Register the function with your cube using <code>cube.register_function()</code></li> <li>Reference the function in your metric expressions using the <code>@function_name</code> syntax (Pandas and Numpy are already registered as pd and np).</li> </ol> <p>This powerful feature allows you to implement virtually any calculation logic while keeping your metric definitions clean and readable.</p> <pre><code>import numpy as np\n\n# Define and register a safe division function\ndef safe_division(numerator, denominator, default=0.0):\n    \"\"\"Safely divide two arrays, handling division by zero\"\"\"\n    result = numerator / denominator\n    return result.replace([np.inf, -np.inf], np.nan).fillna(default)\n\n# Register the function with your hypercube\ncube.register_function(safe_division=safe_division)\n\n# Use it in a metric definition\ncube.define_metric(\n    name='Profit Margin %',\n    expression='@safe_division([revenue] - [cost], [revenue]) * 100',\n    aggregation='mean'\n)\n\n# Another example: categorizing data\ndef categorize_revenue(revenue_values):\n    \"\"\"Categorize revenue into tiers\"\"\"\n    conditions = [\n        revenue_values &lt; 1000,\n        (revenue_values &gt;= 1000) &amp; (revenue_values &lt; 5000),\n        revenue_values &gt;= 5000\n    ]\n    choices = ['Low', 'Medium', 'High']\n    return np.select(conditions, choices, default='Unknown')\n\ncube.register_function(categorize_revenue=categorize_revenue)\n\n# Use for conditional logic - count how many sales fall into each tier\ncube.define_metric(\n    name='Revenue Tier Count',\n    expression='@categorize_revenue([qty] * [price])',\n    aggregation=lambda x: len(x)\n)\n\n# Or get the most common revenue tier\ncube.define_metric(\n    name='Most Common Revenue Tier',\n    expression='@categorize_revenue([qty] * [price])',\n    aggregation=lambda x: x.value_counts().index[0]\n)\n\n# All pandas and numpy functions are already registered functions and can be used right away by calling pd or np\n\n# for example if you have this metric\ncube.define_metric(\n    name='High Value Orders',\n    expression='[order_id]',\n    aggregation='count',\n    row_condition_expression='[price] &gt; 100'\n)\n\n# it can be defined also using numpy inside the expression\n\ncube.define_metric(\n    name='High Value Orders',\n    expression='@np.where([price] &gt; 100, [order_id], np.nan)',\n    aggregation='count'\n)\n\n# Will retrieve the exact same value\n\n# Advanced handling of missing values with @ functions\n# While the basic fillna parameter fills all metric columns with the same value:\ncube.define_metric(\n    name='Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    fillna=0  # Fills both [qty] and [price] with 0\n)\n\n# For column-specific NA handling, use @ functions directly in the expression:\ncube.define_metric(\n    name='Revenue with Custom NA Handling',\n    expression='@pd.Series([qty]).fillna(1) * @pd.Series([price]).fillna(0)',\n    aggregation='sum'  # Fills [qty] with 1 and [price] with 0\n)\n\n# Using np.where for conditional NA handling:\ncube.define_metric(\n    name='Revenue with Conditional Defaults',\n    expression='@np.where(@pd.isnull([qty]), 0 , [qty])',\n    aggregation='sum'\n)\n\n# You can inspect available registered functions:\ncube.registered_functions\n</code></pre> <p>Note on Function Handling</p> <p>Metric aggregation allows you to pass custom functions directly while expression functions need to be registered on the hypercube first and referenced with <code>@function_name</code>. This difference exists due to their roles in the processing pipeline:</p> <ul> <li>Expression functions operate inside within dataframe rows before aggregation</li> <li>Aggregation functions work outside on the grouped data</li> </ul>"},{"location":"concepts/queries/","title":"Understanding Queries","text":"<p>Queries in Cube Alchemy bring together metrics and dimensions to answer specific questions.</p> <p>A query consists of three key components:</p> <ul> <li> <p>Query name: A unique identifier for referencing the query</p> </li> <li> <p>Dimensions: Columns to group by (the \"by what\" in your analysis)</p> </li> <li> <p>Metrics: Measures to calculate (the \"what\" in your analysis)</p> </li> </ul> <pre><code># Define metrics\ncube.define_metric(name='Revenue', expression='[qty] * [price]', aggregation='sum')\ncube.define_metric(name='Order Count', expression='[order_id]', aggregation='count')\n\n# Define a query by region and product category\ncube.define_query(\n    name=\"regional_sales\",\n    dimensions={'region', 'category'},\n    metrics=['Revenue', 'Order Count']\n)\n\n# Execute the query\nresult = cube.query(\"regional_sales\")\n</code></pre>"},{"location":"concepts/queries/#execution-pipeline","title":"Execution Pipeline","text":"<p>When you run a query, Cube Alchemy processes your data in a clear, ordered pipeline:</p> <pre><code>flowchart LR\n  A[\"Apply Context State Filters\"] --&gt; B[\"Fetch Dimensions\"]\n  B --&gt; C[\"Calculate Metrics (Aggregations)\"]\n  C --&gt; D[\"Compute Post-Aggregation Metrics\"]\n  D --&gt; E[\"Apply HAVING Filter\"]\n  E --&gt; F[\"Apply SORT\"]\n  F --&gt; G[\"Return Final Result\"]</code></pre>"},{"location":"concepts/queries/#query-types","title":"Query Types","text":"<p>Queries must contain either dimensions, metrics, or both (a query cannot lack both).</p>"},{"location":"concepts/queries/#dimension-only-queries","title":"Dimension-Only Queries","text":"<p>When you only need to see what unique dimension combinations:</p> <pre><code># Define a query with only dimensions\ncube.define_query(\n    name=\"dimension_combinations\",\n    dimensions={'region', 'category'}\n)\n\n# Get all unique region/category combinations\ncombinations = cube.query(\"dimension_combinations\")\n</code></pre>"},{"location":"concepts/queries/#metric-only-queries","title":"Metric-Only Queries","text":"<p>When you need to calculate global aggregates:</p> <pre><code># Define metrics\ncube.define_metric(name='Total Revenue', expression='[qty] * [price]', aggregation='sum')\ncube.define_metric(name='Total Orders', expression='[order_id]', aggregation='count')\n\n# Define a query with no dimensions\ncube.define_query(\n    name=\"global_totals\",\n    metrics=['Total Revenue', 'Total Orders']\n)\n\n# Execute the query\nglobal_results = cube.query(\"global_totals\")\n</code></pre>"},{"location":"concepts/queries/#computed-metrics-and-having","title":"Computed Metrics and HAVING","text":"<p>Computed metrics are calculated after base metrics are aggregated. Define them once, then reference by name in queries.</p> <pre><code># Define base metrics\ncube.define_metric(name='Cost',   expression='[cost]',           aggregation='sum')\ncube.define_metric(name='Margin', expression='[qty] * [price] - [cost]', aggregation='sum')\n\n# Define a computed metric (post-aggregation)\ncube.define_computed_metric(\n    name='Margin %',\n    expression='[Margin] / [Cost] * 100',\n    fillna=0\n)\n\n# Use computed metric by name and add a HAVING filter\ncube.define_query(\n    name='margin_by_product',\n    dimensions={'product'},\n    metrics=['Margin', 'Cost'],\n    computed_metrics=['Margin %'],\n    having='[Margin %] &gt;= 20'\n)\n\ndf = cube.query('margin_by_product')\n</code></pre> <p>Notes:</p> <ul> <li> <p>Use [Column] syntax in expressions; registered functions are available as @name.</p> </li> <li> <p>Computed metrics reference columns present in the aggregated result (metrics and dimensions).</p> </li> </ul>"},{"location":"concepts/queries/#working-with-filters","title":"Working with Filters","text":"<p>Queries automatically respect all active filters on your hypercube, allowing you to:</p> <ol> <li>Define a query once</li> <li>Apply different filters</li> <li>Execute the same query to see different filtered views of your data</li> </ol> <pre><code># Define your query\ncube.define_query(\n    name=\"product_sales\",\n    dimensions={'region', 'product_category'},\n    metrics=['Revenue', 'Order Count']\n)\n\n# Get unfiltered results\nunfiltered_results = cube.query(\"product_sales\")\n\n# Apply filters\ncube.filter({'product_type': ['Electronics', 'Home']})\n\n# Get filtered results using the same query\nfiltered_results = cube.query(\"product_sales\")\n</code></pre>"},{"location":"concepts/relationships/","title":"Relationships in Cube Alchemy","text":"<p>Cube Alchemy uses implicit relationships, meaning DataFrames automatically connect to each other through shared column names.</p> <p>These relationships enable you to work with your newly connected data declaratively. Cube Alchemy handles all the necessary data operations under the hood.</p>"},{"location":"concepts/relationships/#single-path","title":"Single Path","text":"<p>Cube Alchemy ensures there is exactly one bidirectional path between any two tables in your data model. This creates a clear, unambiguous way to traverse from one table to another.</p>"},{"location":"concepts/relationships/#composite-key","title":"Composite Key","text":"<p>When tables share multiple columns, Cube Alchemy automatically creates bridges to connect them:</p> <ol> <li>Detection: It identifies sets of shared columns between tables</li> <li>Composite Key Creation: A single key is generated from these shared columns, autonumbered to improve performance on the upcoming operations.</li> <li>Composite Tables: New tables are created to store only the shared columns along with the generated keys</li> <li>Column Renaming: The original shared columns in source tables are renamed with a format of <code>column_name &lt;table_name&gt;</code></li> <li>Key Addition: The newly created composite keys are added to the original tables</li> </ol> <p>This process consolidates complex relationships into simple, efficient connections while preserving all the original data.</p>"},{"location":"concepts/relationships/#cardinality","title":"Cardinality","text":"<p>Cube Alchemy takes a flexible approach:</p> <ul> <li> <p>It does not enforce specific cardinality constraints like one-to-many or many-to-one; many-to-many is the default relationship.</p> </li> <li> <p>You should understand your data's natural cardinality to avoid unexpected results in your aggregations (e.g., due to row duplication).</p> </li> </ul> <p>Fact and Dimension Tables</p> <p>As a data analyst you might be already familiar with dimensional modeling principles used in data warehousing and business intelligence. These typically involve:</p> <ul> <li> <p>Fact tables: Contain quantitative data that represent events or transactions (sales, orders, shipments).</p> </li> <li> <p>Dimension tables: Provide descriptive context through attributes that answer the \"who, what, where, when, why, and how\" questions about your data (customers, products, locations, time periods).</p> </li> </ul> <p>Please bear in mind that Cube Alchemy's approach is flexible regarding relationships between your data rather than enforcing a specific modeling paradigm:</p> <ul> <li>Cube Alchemy does not explicitly enforce Fact and Dimension table distinctions.</li> <li>Every single column provided in the DataFrames you use to define a Hypercube will be available as a dimension, and can also be used to create metrics and filters.</li> <li>The framework is design-agnostic. It's up to you as the analyst to define which tables serve as Facts or Dimensions in your specific context.</li> </ul>"},{"location":"concepts/relationships/#visualization","title":"Visualization","text":"<p>When setting up your hypercube, it's a good idea to inspect the relationship graph to confirm how tables connect and where composite keys have been introduced.  <pre><code># Default view shows renamed columns (column_name &lt;table_name&gt;)\ncube.visualize_graph()\n\n# For cleaner display without table name suffixes\ncube.visualize_graph(full_column_names=False, w=16, h=12)\n</code></pre> This visualization helps you understand the automatic transformations that Cube Alchemy has applied.</p> <p>Note: If the displayed graph doesn't look so good try a couple of times or adjust the size.</p>"},{"location":"concepts/workflow/","title":"Workflow","text":"<pre><code>flowchart LR\n  A[\"Load DataFrames\"] --&gt; B[\"Build Hypercube\"]\n  B --&gt; C[\"Define Metrics\"]\n  C --&gt; D[\"Define Queries\"]\n  D --&gt; E[\"Execute Queries\"]\n  E --&gt; F[\"Update Context State (Apply or Remove Filters)\"]\n  F --&gt; E</code></pre> <p>Cube Alchemy's workflow is intuitive and powerful: load your data, build a unified hypercube structure, define your metrics, and create reusable queries. The stateful architecture allows you to execute queries and apply filters in an iterative process.</p>"}]}