{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Build a Powerful Hypercube","text":"<p>More analysis, less plumbing. Cube Alchemy automatically transforms your disconnected pandas DataFrames into a unified, multidimensional data model.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome. Please open an issue to discuss major changes first. PRs should include concise descriptions and, where applicable, tests or examples.</p>"},{"location":"examples/","title":"Examples","text":"<p>(Reserved for advanced, end-to-end examples. To be added.)</p>"},{"location":"installation/","title":"Installation","text":"<p>Requires Python 3.8+.</p> <p></p> <pre><code>python -m venv venv\nvenv\\Scripts\\activate\npip install cube-alchemy\n</code></pre>"},{"location":"overview/","title":"Overview","text":"<p>Cube Alchemy transforms your pandas DataFrames into a powerful hypercube, creating a unified semantic layer for multidimensional analysis. This allows you to move from disconnected tables to a coherent analytical model where you can explore data simply and declaratively.</p>"},{"location":"overview/#core-capabilities","title":"Core Capabilities","text":"<ul> <li> <p>Automatic Relationships:  Discovers relationships between your DataFrames by matching shared column names.</p> <ul> <li>Complex Relationship Handling: Handles composite keys and complex relationships transparently.</li> </ul> </li> <li> <p>Multidimensional Analytics: Slice, dice, and aggregate your data across any dimension with consistent, reusable metrics and queries that reduce boilerplate code.</p> </li> <li> <p>Stateful Analysis: Maintain a filtering context across queries to easily compare different scenarios.</p> </li> <li> <p>Interactive &amp; Scalable: Works seamlessly in notebook and data apps (Streamlit/Panel).</p> </li> <li> <p>Framework-Agnostic Visualization: Flexible plotting system with a clean renderer interface that works with any visualization framework.</p> </li> </ul>"},{"location":"overview/#the-semantic-layer","title":"The Semantic Layer","text":"<p>Map your data into a clear and consistent set of analytical assets to work with your hypercube:</p> <ul> <li> <p>Dimensions: The \"by\" of your analysis\u2014the entities you use to slice and dice data (e.g., <code>Customer</code>, <code>Region</code>, <code>Product</code>).</p> </li> <li> <p>Metrics: The key performance indicators (KPIs) you measure (e.g., <code>Total Revenue</code>, <code>Conversion Rate</code>, <code>Average Order Value</code>).</p> </li> <li> <p>Queries: The questions you ask of your data, combining metrics and dimensions to produce insights (e.g., Revenue by Region over Time).</p> </li> <li> <p>Plot Configurations: Visualization definitions that specify how query results should be displayed, with support for multiple views of the same data.</p> </li> </ul>"},{"location":"overview/#why-it-matters","title":"Why It Matters","text":"<p>Build faster, reliable analytics with a fraction of the effort.</p> <ul> <li> <p>Accelerate Insights: Get into deep analysis in minutes. Relationships are discovered automatically, not manually coded.</p> </li> <li> <p>Simplify Complexity: Replace ad-hoc joins and messy code with clean, declarative queries that are easy to read and maintain.</p> </li> <li> <p>Ensure Consistency: Standardized metrics and a central data model guarantee that everyone gets reliable, consistent results.</p> </li> <li> <p>Integrate Seamlessly: Designed to work with Streamlit and other Python-based frameworks for building interactive data applications.</p> </li> </ul>"},{"location":"streamlit/","title":"Streamlit Integration","text":"<p>Cube Alchemy integrates seamlessly with Streamlit to create interactive data applications. The new plotting system makes this integration even more powerful and flexible.</p>"},{"location":"streamlit/#basic-integration","title":"Basic Integration","text":""},{"location":"streamlit/#1-set-up-your-streamlit-app","title":"1. Set Up Your Streamlit App","text":"<p>Start by importing the necessary libraries and setting up your Streamlit app:</p> <pre><code>import streamlit as st\nimport pandas as pd\nfrom cube_alchemy.core.hypercube import Hypercube\nfrom cube_alchemy.plot_renderer import PlotRenderer\n\n# Set page config\nst.set_page_config(\n    page_title=\"Cube Alchemy Analytics\",\n    page_icon=\"\ud83d\udcca\",\n    layout=\"wide\"\n)\n</code></pre>"},{"location":"streamlit/#2-create-a-streamlitrenderer","title":"2. Create a StreamlitRenderer","text":"<p>Create a custom renderer for Streamlit by implementing the PlotRenderer interface:</p> <pre><code>class StreamlitRenderer(PlotRenderer):\n    \"\"\"Renderer for Streamlit visualizations\"\"\"\n\n    def __init__(self, height=None, use_container_width=True):\n        self.height = height\n        self.use_container_width = use_container_width\n\n    def render(self, data, plot_config, **kwargs):\n        # Get configuration options\n        plot_type = plot_config.get('plot_type', 'bar')\n        x_column = plot_config.get('x_column')\n        y_column = plot_config.get('y_column')\n        title = plot_config.get('title')\n        orientation = plot_config.get('orientation', 'vertical')\n\n        # Display title if provided\n        if title:\n            st.subheader(title)\n\n        # Handle different plot types\n        if plot_type == 'bar':\n            return st.bar_chart(\n                data.set_index(x_column)[y_column],\n                height=self.height,\n                use_container_width=self.use_container_width\n            )\n        elif plot_type == 'line':\n            return st.line_chart(\n                data.set_index(x_column)[y_column],\n                height=self.height,\n                use_container_width=self.use_container_width\n            )\n        elif plot_type == 'area':\n            return st.area_chart(\n                data.set_index(x_column)[y_column],\n                height=self.height,\n                use_container_width=self.use_container_width\n            )\n        else:\n            st.error(f\"Plot type '{plot_type}' not supported\")\n            return None\n</code></pre>"},{"location":"streamlit/#3-load-your-hypercube","title":"3. Load Your Hypercube","text":"<p>Load your hypercube, typically from a pickle file for a Streamlit app:</p> <pre><code>@st.cache_resource  # Cache the cube to improve performance\ndef load_cube():\n    cube = Hypercube.load_pickle(\"path_to_your_cube.pkl\")\n    # Important: Set context state after loading\n    cube.set_context_state('Default')\n    # If your app defines custom functions, re-register them here, e.g.:\n    # from my_udfs import normalize\n    # cube.add_functions(normalize=normalize)\n    return cube\n\n# Load the cube\ncube = load_cube()\n</code></pre>"},{"location":"streamlit/#4-create-interactive-controls","title":"4. Create Interactive Controls","text":"<p>Add interactive controls to filter your data:</p> <pre><code># Add dimension filters in the sidebar\nst.sidebar.header(\"Filters\")\n\n# Get all dimensions\nall_dims = cube.dimensions.keys()\nselected_dims = st.sidebar.multiselect(\"Filter by dimensions\", options=all_dims)\n\n# For each selected dimension, add a filter control\nfilters = {}\nfor dim in selected_dims:\n    values = cube.dimensions([dim])[dim]\n    options = values.dropna().unique().tolist()\n    selected = st.sidebar.multiselect(f\"Select {dim}\", options=options)\n    if selected:\n        filters[dim] = selected\n\n# Apply filters if any\nif filters:\n    cube.filter(filters)\n</code></pre>"},{"location":"streamlit/#5-display-visualizations","title":"5. Display Visualizations","text":"<p>Display visualizations using the renderer and plot configurations:</p> <pre><code># Create tabs for different views\ntab_viz, tab_data = st.tabs([\"Visualization\", \"Data\"])\n\nwith tab_viz:\n    st.header(\"Query Visualization\")\n\n    # Select which query to visualize\n    queries = list(cube.queries.keys())\n    selected_query = st.selectbox(\"Select Query\", options=queries)\n\n    # Get available plot names\n    plot_names = cube.list_plots(selected_query)\n\n    if plot_names:\n        # Let user select a plot configuration\n        selected_plot = st.selectbox(\n            \"Select Plot Configuration\", \n            options=plot_names\n        )\n\n        # Create renderer\n        renderer = StreamlitRenderer(height=400)\n\n        # Plot using selected configuration\n        cube.plot(selected_query, renderer=renderer, plot_name=selected_plot)\n    else:\n        st.warning(f\"No plot configurations for query '{selected_query}'\")\n\n        # Show raw data as fallback\n        result = cube.query(selected_query)\n        st.dataframe(result)\n\nwith tab_data:\n    st.header(\"Query Data\")\n    result = cube.query(selected_query)\n    st.dataframe(result)\n</code></pre>"},{"location":"streamlit/#advanced-integration","title":"Advanced Integration","text":""},{"location":"streamlit/#interactive-plot-configuration","title":"Interactive Plot Configuration","text":"<p>Allow users to customize plot configurations on the fly:</p> <pre><code>with st.expander(\"Customize Plot\"):\n    # Get the current plot config\n    plot_config = cube.get_plot_config(selected_query, selected_plot)\n\n    # Allow customization\n    col1, col2 = st.columns(2)\n\n    with col1:\n        plot_type = st.selectbox(\n            \"Plot Type\",\n            options=[\"bar\", \"line\", \"area\"],\n            index=[\"bar\", \"line\", \"area\"].index(plot_config.get(\"plot_type\", \"bar\"))\n        )\n\n        title = st.text_input(\"Title\", value=plot_config.get(\"title\", \"\"))\n\n    with col2:\n        # Allow selecting different metrics\n        query_def = cube.get_query(selected_query)\n        available_metrics = query_def[\"metrics\"] + query_def[\"derived_metrics\"]\n\n        y_column = st.selectbox(\n            \"Y-Axis (Metric)\",\n            options=available_metrics,\n            index=available_metrics.index(plot_config.get(\"y_column\")) if plot_config.get(\"y_column\") in available_metrics else 0\n        )\n\n        orientation = st.selectbox(\n            \"Orientation\",\n            options=[\"vertical\", \"horizontal\"],\n            index=[\"vertical\", \"horizontal\"].index(plot_config.get(\"orientation\", \"vertical\"))\n        )\n\n# Use customized configuration        \ncube.plot(\n    selected_query, \n    renderer=renderer,\n    plot_type=plot_type,\n    y_column=y_column,\n    title=title,\n    orientation=orientation\n)\n</code></pre>"},{"location":"streamlit/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Cache your cube: Use <code>@st.cache_resource</code> to prevent reloading the cube on every interaction.</p> </li> <li> <p>Reset context state: Always set a context state after loading a pickled cube.</p> </li> <li> <p>Handle errors gracefully: Check for missing plot configurations and provide fallbacks.</p> </li> <li> <p>Provide data transparency: Always give users access to the raw data behind visualizations.</p> </li> <li> <p>Use multiple views: Leverage the multiple plot configurations to offer different perspectives on the same data.</p> </li> </ol>"},{"location":"api/analytics_specs/","title":"Hypercube Analytics Specs","text":"<p>Inspect and manage analytics assets (dimensions, metrics, derived metrics, and queries) defined in the hypercube.</p>"},{"location":"api/analytics_specs/#define_metric","title":"define_metric","text":"<pre><code>define_metric(\n    name: Optional[str] = None,\n    expression: Optional[str] = None,\n    aggregation: Optional[Union[str, Callable[[Any], Any]]] = None,\n    metric_filters: Optional[Dict[str, Any]] = None,\n    row_condition_expression: Optional[str] = None,\n    context_state_name: str = 'Default',\n    ignore_dimensions: bool = False,\n    ignore_context_filters: bool = False,\n    fillna: Optional[Any] = None,\n    nested: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Define a base metric used by queries. The method constructs an internal <code>Metric</code> object, infers table/column contexts and registers dependency edges so queries referencing the metric can be refreshed automatically when the metric is added.</p> <p>Parameters:</p> <ul> <li> <p><code>name</code>: Optional string name for the metric. If not provided, the Metric class may infer or raise according to its own rules.</p> </li> <li> <p><code>expression</code>: The expression string describing how the metric is computed (column names, functions, etc.).</p> </li> <li> <p><code>aggregation</code>: Aggregation to apply (string like 'sum', or a callable aggregator).</p> </li> <li> <p><code>metric_filters</code>: Optional dict of filters to apply when computing the metric.</p> </li> <li> <p><code>row_condition_expression</code>: Optional expression to filter rows before aggregation.</p> </li> <li> <p><code>context_state_name</code>: Context state to attach the metric to (defaults to 'Default').</p> </li> <li> <p><code>ignore_dimensions</code>: If True, dimensions will be ignored when computing the metric.</p> </li> <li> <p><code>ignore_context_filters</code>: If True, context filters are ignored for this metric.</p> </li> <li> <p><code>fillna</code>: Optional value to fill missing results.</p> </li> <li> <p><code>nested</code>: Optional nested configuration used by composite/nested metrics.</p> </li> </ul>"},{"location":"api/analytics_specs/#get_dimensions","title":"get_dimensions","text":"<pre><code>get_dimensions() -&gt; List[str]\n</code></pre> <p>Return all available dimension columns across all tables.</p> <p>Note: to fetch distinct values for a single dimension, see Query API: <code>dimension(dimension: str) -&gt; List[str]</code>.</p>"},{"location":"api/analytics_specs/#get_metrics","title":"get_metrics","text":"<pre><code>get_metrics() -&gt; Dict[str, Any]\n</code></pre> <p>Retrieve all defined base metrics. </p> <p>Returns:</p> <ul> <li>Dictionary of metrics with their details (name, expression, aggregation, and other properties)</li> </ul>"},{"location":"api/analytics_specs/#get_metric","title":"get_metric","text":"<pre><code>get_metric(metric: str) -&gt; Dict[str, Any]\n</code></pre> <p>Return a single base metric definition.</p>"},{"location":"api/analytics_specs/#get_derived_metrics","title":"get_derived_metrics","text":"<pre><code>get_derived_metrics() -&gt; Dict[str, Any]\n</code></pre> <p>Retrieve all persisted derived metrics.</p> <p>Returns:</p> <ul> <li>Dictionary mapping derived metric names to specs: expression, optional fillna, and referenced columns</li> </ul>"},{"location":"api/analytics_specs/#get_derived_metric","title":"get_derived_metric","text":"<pre><code>get_derived_metric(derived_metric: str) -&gt; Dict[str, Any]\n</code></pre> <p>Return a single derived metric definition.</p>"},{"location":"api/analytics_specs/#define_derived_metric","title":"define_derived_metric","text":"<pre><code>define_derived_metric(name: str, expression: str, fillna: Optional[Any] = None) -&gt; None\n</code></pre> <p>Persist a post-aggregation derived metric. Derived metrics are evaluated after base metric aggregation and can reference aggregated columns or dimensions using the library's computed-column syntax.</p> <p>Parameters:</p> <ul> <li> <p><code>name</code>: Name of the derived metric (required).</p> </li> <li> <p><code>expression</code>: Expression string for the derived metric (required).</p> </li> <li> <p><code>fillna</code>: Optional value to use to fill missing values after evaluation.</p> </li> </ul>"},{"location":"api/analytics_specs/#get_queries","title":"get_queries","text":"<pre><code>get_queries() -&gt; Dict[str, Any]\n</code></pre> <p>Returns:</p> <ul> <li>Dictionary of queries with their dimensions, metrics, and display options</li> </ul>"},{"location":"api/analytics_specs/#get_query","title":"get_query","text":"<pre><code>get_query(query: str) -&gt; Dict[str, Any]\n</code></pre> <p>Return the definition for a single query (dimensions, metrics, derived_metrics, and options like having and sort).</p> <p>Fields include: - <code>dimensions</code>: List[str]</p> <ul> <li> <p><code>metrics</code>: List[str]</p> </li> <li> <p><code>derived_metrics</code>: List[str]</p> </li> <li> <p><code>having</code>: Optional[str]</p> </li> <li> <p><code>sort</code>: List[Tuple[str, str]]</p> </li> <li> <p><code>drop_null_dimensions</code>: bool</p> </li> <li> <p><code>drop_null_metric_results</code>: bool</p> </li> </ul> <p>See Query API for creation and execution semantics.</p>"},{"location":"api/analytics_specs/#delete_query","title":"delete_query","text":"<pre><code>delete_query(name: str) -&gt; None\n</code></pre> <p>Remove a query definition and its dependency edges. Any linked plots are detached.</p>"},{"location":"api/analytics_specs/#define_query","title":"define_query","text":"<pre><code>define_query(\n    name: str,\n    dimensions: set[str] = {},\n    metrics: List[str] = [],\n    derived_metrics: List[str] = [],\n    having: Optional[str] = None,\n    sort: List[Tuple[str, str]] = [],\n    drop_null_dimensions: bool = False,\n    drop_null_metric_results: bool = False,\n)\n</code></pre> <p>Create or redefine a query specification. The implementation precomputes \"hidden\" base and derived metrics required by requested derived metrics, HAVING expressions, and SORT columns. It also registers dependency edges so queries will be auto-refreshed when referenced metrics or derived metrics are later defined.</p> <p>Parameters: - <code>name</code>: Query identifier. - <code>dimensions</code>: Ordered set/list of dimension column names used by the query. - <code>metrics</code>: List of base metric names to compute. - <code>derived_metrics</code>: List of derived metric names to compute (post-aggregation). - <code>having</code>: Optional HAVING expression applied after aggregation. - <code>sort</code>: List of (column, direction) tuples describing sort order. - <code>drop_null_dimensions</code>: If True, rows with nulls in dimension columns are dropped. - <code>drop_null_metric_results</code>: If True, rows with null metric results are dropped.</p>"},{"location":"api/analytics_specs/#delete_metric","title":"delete_metric","text":"<pre><code>delete_metric(name: str) -&gt; None\n</code></pre> <p>Remove a base metric. Dependent queries will still reference the name and appear missing until redefined.</p>"},{"location":"api/analytics_specs/#delete_derived_metric","title":"delete_derived_metric","text":"<pre><code>delete_derived_metric(name: str) -&gt; None\n</code></pre> <p>Remove a derived metric. Dependent queries will still reference the name and appear missing until redefined.</p>"},{"location":"api/analytics_specs/#debug_dependencies","title":"debug_dependencies","text":"<pre><code>debug_dependencies() -&gt; Dict[str, List[List[str]]]\n</code></pre> <p>Return a snapshot of dependency edges from sources (metric/derived metric/query names) to their dependents.</p>"},{"location":"api/analytics_specs/#debug_missing_dependencies","title":"debug_missing_dependencies","text":"<pre><code>debug_missing_dependencies() -&gt; Dict[str, List[List[str]]]\n</code></pre> <p>Return only unresolved sources (not a defined base metric, not a defined derived metric, not a known dimension, and not a query). Useful to identify missing definitions referenced by queries or plots.</p>"},{"location":"api/filter_methods/","title":"Filter Methods","text":""},{"location":"api/filter_methods/#filter","title":"filter","text":"<pre><code>filter(\n  criteria: Dict[str, List[Any]], \n  context_state_name: str = 'Default',\n  is_reset: bool = False,\n  save_state: bool = True,\n) -&gt; bool\n</code></pre> <p>Apply filters to a context state. Returns True on success.</p>"},{"location":"api/filter_methods/#remove_filter","title":"remove_filter","text":"<pre><code>remove_filter(\n  dimensions: List[str],\n  context_state_name: str = 'Default',\n  is_reset: bool = False,\n) -&gt; bool\n</code></pre> <p>Remove filters for the specified dimensions from a context state.</p>"},{"location":"api/filter_methods/#reset_filters","title":"reset_filters","text":"<pre><code>reset_filters(\n  direction: str = 'backward',\n  context_state_name: str = 'Default'\n) -&gt; bool\n</code></pre> <p>Reset filters using undo/redo or clear all filters. Direction: 'backward', 'forward', or 'all'.</p>"},{"location":"api/filter_methods/#get_filters","title":"get_filters","text":"<pre><code>get_filters(\n  off_set: int = 0,\n  context_state_name: str = 'Default'\n) -&gt; Dict[str, List[Any]]\n</code></pre> <p>Return the aggregated active filters up to the current history pointer plus off_set.</p>"},{"location":"api/filter_methods/#get_filtered_dimensions","title":"get_filtered_dimensions","text":"<pre><code>get_filtered_dimensions(\n  off_set: int = 0,\n  context_state_name: str = 'Default'\n) -&gt; List[str]\n</code></pre> <p>Return only the dimension names that are currently filtered (order preserved, no duplicates).</p>"},{"location":"api/filter_methods/#set_context_state","title":"set_context_state","text":"<pre><code>set_context_state(\n  context_state_name: str,\n  base_context_state_name: str = 'Unfiltered'\n) -&gt; bool\n</code></pre> <p>Create a new context state cloned from base_context_state_name.</p> <p>Notes: - When creating a new context state, the implementation initializes <code>applied_filters[context_state_name]</code> as an empty list and sets <code>filter_pointer[context_state_name]</code> to 0. The new context is a copy of the specified base context state.</p>"},{"location":"api/hypercube/","title":"Hypercube API","text":"<p>The Hypercube class is the central component of Cube Alchemy, providing methods for creating, querying, and analyzing multidimensional data.</p>"},{"location":"api/hypercube/#initialization","title":"Initialization","text":"<p>The Hypercube can be initialized in two ways:</p> <pre><code># Option 1: Initialize with data (recommended for immediate use)\nHypercube(\n    tables: Optional[Dict[str, pd.DataFrame]] = None,\n    rename_original_shared_columns: bool = True,\n    normalized_core: bool = False,\n    *,\n    apply_composite: bool = True,\n    validate: bool = True,\n    to_be_stored: bool = False,\n    logger: Optional[Union[bool, logging.Logger]] = None,\n    validator_cls: Optional[Type[SchemaValidator]] = None,\n    bridge_factory_cls: Optional[Type[CompositeBridgeGenerator]] = None,\n    function_registry: Optional[Dict[str, Any]] = None,\n)\n\n# Option 2: Initialize empty and load data later\nHypercube()\n\nload_data(\n    tables: Dict[str, pd.DataFrame],\n    rename_original_shared_columns: bool = True,\n    apply_composite: bool = True,\n    validate: bool = True,\n    to_be_stored: bool = False,\n    reset_specs: bool = False,\n)\n</code></pre> <p>The <code>load_data()</code> method can also be used to reload or update data in an existing hypercube.</p> <p>Parameters:</p> <ul> <li> <p><code>tables</code>: Dictionary mapping table names to pandas DataFrames</p> </li> <li> <p><code>rename_original_shared_columns</code>: Controls what happens to shared columns in source tables.  </p> <ul> <li> <p>True (default): keep them, renamed as <code>&lt;column&gt; (&lt;table_name&gt;)</code>. Enables per\u2011table counts/aggregations.  </p> </li> <li> <p>False: drop them from source tables (values remain in link tables). Saves time and memory if per\u2011table analysis isn\u2019t needed.  </p> </li> </ul> </li> <li> <p><code>to_be_stored</code>: Set to True if the hypercube will be serialized/stored (skips Default context state creation)</p> </li> <li> <p><code>normalized_core</code>: Controls the core data storage strategy for the hypercube, affecting performance characteristics based on your data and query patterns.</p> <ul> <li> <p><code>False</code> (default): Normalized core - The core stores only keys and indexes from the schema. Each query dynamically joins required dimensions and metrics from their source tables. This approach uses less memory but requires joins during query execution.</p> </li> <li> <p><code>True</code>: Denormalized core - All schema tables are joined together into one large table stored as the core. Queries fetch data directly from this pre-joined structure without additional joins. This approach uses more memory but can be faster for queries that span multiple tables.</p> </li> </ul> <p>Performance trade-offs:</p> <p>The optimal choice depends on your data size, relationship complexity, and query patterns. For large datasets with many relationships, the normalized approach may be more efficient. For smaller datasets with frequent cross-table analysis, the denormalized approach may perform better.</p> </li> <li> <p><code>reset_specs</code> (only load_data method): Whether to clear all existing analytics and plotting definitions (metrics, derived metrics, queries, plots, transformations) and the function registry before loading new ones from the Catalog Source.</p> </li> <li> <p><code>logger</code> (only constructor): Controls instance logging.</p> <ul> <li><code>True</code>: enable default Python logging (INFO) with format <code>%(levelname)s %(name)s: %(message)s</code>.</li> <li><code>logging.Logger</code>: use the provided logger instance.</li> <li><code>False</code>: silence this instance (no propagation; internal NullHandler).</li> <li><code>None</code>/omitted: no global config; a module-named logger is used and inherits app-level settings.</li> </ul> </li> </ul> <p>Examples:</p> <pre><code>import pandas as pd\nfrom cube_alchemy import Hypercube\nimport logging\n\n# Option 1: Initialize with data (keep renamed shared columns)\ncube1 = Hypercube({\n    'Product': products_df,\n    'Customer': customers_df,\n    'Sales': sales_df\n}, rename_original_shared_columns=True)\n\n# Option 2: Initialize empty first, then load data\ncube2 = Hypercube()\ncube2.load_data({\n    'Product': products_df,\n    'Customer': customers_df,\n    'Sales': sales_df\n}, rename_original_shared_columns=False)\n\n# Reload data in an existing hypercube (e.g., when data is updated)\ncube1.load_data({\n    'Product': updated_products_df,\n    'Customer': updated_customers_df,\n    'Sales': updated_sales_df\n})\n\n# Reset definitions when loading a new data schema\ncube2.load_data(new_data, reset_specs=True)\n\n# Enable default logging quickly\ncube3 = Hypercube({'Sales': sales_df}, logger=True)\n</code></pre>"},{"location":"api/hypercube/#reset_specs","title":"reset_specs","text":"<pre><code>reset_specs() -&gt; None\n</code></pre> <p>Clear all analytics and plotting definitions and the function registry:</p> <ul> <li>Metrics and derived metrics</li> <li>Queries</li> <li>Plots and transformations</li> <li>Function registry entries (custom functions)</li> </ul> <p>Use this to start from a clean slate before reloading definitions or importing from a model catalog.</p> <p>Example:</p> <pre><code>cube.reset_specs()\ncube.load_from_model_catalog(reset_specs=True) # Reset the specs and then load from source model catalog (eg. YAML)\n</code></pre>"},{"location":"api/hypercube/#core-methods","title":"Core Methods","text":""},{"location":"api/hypercube/#visualize_graph","title":"visualize_graph","text":"<pre><code>visualize_graph(\n  w: Optional[float] = None,\n  h: Optional[float] = None,\n  full_column_names: bool = False,\n  seed: Optional[int] = None,\n  show: bool = True,\n  return_fig: bool = False,\n) -&gt; Optional[matplotlib.figure.Figure]\n</code></pre> <p>Visualize the relationships between tables as a network graph.</p> <p>Parameters:</p> <ul> <li> <p><code>w</code>: Width of the plot in inches. If None, an automatic size is chosen based on graph size.</p> </li> <li> <p><code>h</code>: Height of the plot in inches. If None, an automatic size is chosen based on graph size.</p> </li> <li> <p><code>full_column_names</code>: Whether to show renamed columns with table reference (e.g., <code>column &lt;table_name&gt;</code>) or just the original column names.</p> </li> <li> <p><code>seed</code>: RNG seed for the spring layout; set a fixed value to make the layout reproducible.</p> </li> <li> <p><code>show</code>: If True, call <code>plt.show()</code> to display the figure.</p> </li> <li> <p><code>return_fig</code>: If True, return the Matplotlib Figure instead of None.</p> </li> </ul> <p>Example:</p> <p><pre><code># Visualize the data model relationships\ncube.visualize_graph()\n\n# Hide renamed column format for cleaner display\ncube.visualize_graph(full_column_names=False)\n</code></pre> Note: If the displayed graph doesn't look so good try a couple of times or adjust the size.</p>"},{"location":"api/hypercube/#relationship_matrix","title":"relationship_matrix","text":"<p><pre><code>get_relationship_matrix(context_state_name: str = 'Unfiltered') -&gt; pd.DataFrame\n</code></pre> Reconstruct the original shared columns across the model to inspect connectivity.</p>"},{"location":"api/hypercube/#get_cardinalities","title":"get_cardinalities","text":"<p><pre><code>get_cardinalities(context_state_name: str = 'Unfiltered', include_inverse: bool = False) -&gt; pd.DataFrame\n</code></pre> Compute relationship cardinalities for shared keys between base tables.</p>"},{"location":"api/hypercube/#shared-columns-counts-and-distincts","title":"Shared columns: counts and distincts","text":"<p>When multiple tables share a column (for example, <code>customer_id</code>), Cube Alchemy builds a link table containing the distinct values of that column across all participating tables. This has two practical implications:</p> <ul> <li> <p>Counting on the shared column name (e.g., <code>customer_id</code>) uses the link table and therefore reflects distinct values in the current filtered context across all tables that share it.</p> </li> <li> <p>Counting on a per-table renamed column (e.g., <code>customer_id &lt;orders&gt;</code> or <code>customer_id &lt;customers&gt;</code>) uses that table\u2019s own column values. The result can differ from the shared-column count because it\u2019s scoped to that single table's values and is not the cross-table distinct set.</p> </li> </ul> <p>Example idea:</p> <ul> <li> <p>Count distinct <code>customer_id</code> (shared) \u2192 distinct customers across all linked tables.</p> </li> <li> <p>Count distinct <code>customer_id &lt;orders&gt;</code> \u2192 distinct customers present in the Orders table specifically.</p> </li> </ul> <p>Choose the one that matches your analytical intent: cross-table distincts via the shared column, or table-specific distincts via the renamed columns. Note: per-table renamed columns are available only when <code>rename_original_shared_columns=True</code>; set it to False to drop them and reduce memory/processing if you don\u2019t need that analysis.</p>"},{"location":"api/hypercube/#persistence-helpers","title":"Persistence helpers","text":""},{"location":"api/hypercube/#save_as_pickle","title":"save_as_pickle","text":"<pre><code>save_as_pickle(\n    path: Optional[Union[str, Path]] = None,\n    *,\n    relative_path: bool = True,\n    pickle_name: str = \"cube.pkl\",\n) -&gt; Path\n</code></pre> <p>Serialize the current Hypercube instance to a pickle file. If <code>path</code> is omitted the working directory is used and <code>pickle_name</code> is written there. The logger is temporarily removed during pickling to avoid capturing non-picklable handlers. The function registry is persisted via import specs so only importable top-level callables are fully restorable.</p>"},{"location":"api/hypercube/#load_pickle-static","title":"load_pickle (static)","text":"<pre><code>@staticmethod\nload_pickle(\n    path: Optional[Union[str, Path]] = None,\n    *,\n    relative_path: bool = True,\n    pickle_name: str = \"cube.pkl\",\n) -&gt; Hypercube\n</code></pre> <p>Load a previously pickled Hypercube. If <code>path</code> points to a directory, <code>pickle_name</code> is appended; if it is a file path it is used directly.</p>"},{"location":"api/metric_methods/","title":"Metric Methods","text":""},{"location":"api/metric_methods/#define_metric","title":"define_metric","text":"<pre><code>define_metric(\n    name: Optional[str] = None,\n    expression: Optional[str] = None,\n    aggregation: Optional[Union[str, Callable[[Any], Any]]] = None,\n    metric_filters: Optional[Dict[str, Any]] = None,\n    row_condition_expression: Optional[str] = None,\n    context_state_name: str = 'Default',\n    ignore_dimensions: bool = False,\n    ignore_context_filters: bool = False,\n    fillna: Optional[Any] = None,\n    nested: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Persist a base metric for later use in queries.</p> <p>Parameters:</p> <ul> <li> <p>name: Label of the resulting column.</p> </li> <li> <p>expression: Formula using [column] references and optional @functions.</p> </li> <li> <p>aggregation: Pandas aggregation name or callable; aliases like avg, count_distinct supported.</p> </li> <li> <p>metric_filters: Filters applied only when evaluating this metric.</p> </li> <li> <p>row_condition_expression: Row filter (DataFrame.query style; use [col] references).</p> </li> <li> <p>context_state_name: Context state to read from.</p> </li> <li> <p><code>ignore_dimensions</code>: True to compute a grand-total metric (ignore query dimensions), or False to compute per-dimension values.</p> </li> <li> <p><code>ignore_context_filters</code>: True to ignore all context filters when computing this metric, or False to respect context filters.</p> </li> <li> <p>fillna: Single value to fill NA on referenced columns before evaluation.</p> </li> <li> <p>nested: Dict with keys: dimensions (str|list[str]); aggregation (str|callable, default 'sum'); compose (str template or callable(row)-&gt;str).</p> </li> </ul> <p>Notes: - Column references must use square brackets: [qty], [price], ...</p> <ul> <li>Aggregation accepts pandas names, callables, lists/tuples/dicts, and common aliases.</li> </ul>"},{"location":"api/metric_methods/#define_derived_metric","title":"define_derived_metric","text":"<pre><code>define_derived_metric(\n    name: str,\n    expression: str,\n    fillna: Optional[Any] = None\n) -&gt; None\n</code></pre> <p>Persist a post-aggregation derived metric computed from aggregated columns (metrics or dimensions).</p> <p>Parameters: - name: Result column label.</p> <ul> <li> <p>expression: Formula using [MetricName] and/or [Dimension] references.</p> </li> <li> <p>fillna: Optional value to temporarily fill NA on referenced columns during evaluation.</p> </li> </ul>"},{"location":"api/model_catalog/","title":"Model Catalog API","text":""},{"location":"api/model_catalog/#set_yaml_model_catalog","title":"set_yaml_model_catalog","text":"<pre><code>set_yaml_model_catalog(\n    path: Optional[str] = None,\n    use_current_directory: bool = True,\n    create_if_missing: bool = True,\n    default_yaml_name: Optional[str] = None,\n    prefer_nested_plots: bool = True,\n    prefer_nested_transformers: bool = True,\n) -&gt; Path\n</code></pre> <p>Attach a YAML file as a source for model definitions and return its resolved path.</p>"},{"location":"api/model_catalog/#get_yaml_path_model_catalog","title":"get_yaml_path_model_catalog","text":"<pre><code>get_yaml_path_model_catalog() -&gt; Optional[Path]\n</code></pre> <p>Return the currently attached YAML path, if any.</p>"},{"location":"api/model_catalog/#set_model_catalog","title":"set_model_catalog","text":"<pre><code>set_model_catalog(sources: Iterable[Source], repo: Optional[Repository] = None) -&gt; None\n</code></pre> <p>Attach multiple sources (merged by Catalog) with an optional repository (defaults to in-memory).</p>"},{"location":"api/model_catalog/#load_from_model_catalog","title":"load_from_model_catalog","text":"<pre><code>load_from_model_catalog(\n    kinds: Optional[Iterable[str]] = None,\n    reset_specs: bool = False,\n    clear_repo: bool = False,\n    reload_sources: bool = True,\n) -&gt; None\n</code></pre> <p>Load from sources into the Catalog, then apply to the cube. Order: metrics -&gt; derived_metrics -&gt; queries -&gt; plots -&gt; transformers.</p> <ul> <li><code>reset_specs</code>: If True, clear all current metrics, derived metrics, queries, plots, transformers, and the function registry on the cube before applying catalog definitions.</li> </ul>"},{"location":"api/model_catalog/#save_to_model_catalog","title":"save_to_model_catalog","text":"<pre><code>save_to_model_catalog(prefer_nested_plots: bool = True, prefer_nested_transformers: bool = True) -&gt; Optional[Path]\n</code></pre> <p>Save current cube definitions (metrics, derived metrics, queries, plots, transformers) through attached sources. Returns the path to the YAML file if a YAML source was attached via <code>set_yaml_model_catalog(...)</code>, otherwise may return None.</p> <p>Notes:</p> <ul> <li> <p>The Catalog repository acts as the in-memory store; sources decide persistence (e.g., YAML).</p> </li> <li> <p>Plots and transformers are serialized with their associated query reference.</p> </li> </ul> <p>Notes on nesting preferences:</p> <ul> <li> <p><code>prefer_nested_plots</code>: When True, plots will be nested under their parent queries in YAML output (default True).</p> </li> <li> <p><code>prefer_nested_transformers</code>: When True, transformers will be nested under parent queries in YAML output (default True).</p> </li> </ul>"},{"location":"api/plotting_system/","title":"Plotting System","text":"<p>Manage plot configurations per query and render via pluggable renderers.</p>"},{"location":"api/plotting_system/#define_plot","title":"define_plot","text":"<pre><code>define_plot(\n    query_name: str,\n    plot_name: Optional[str] = None,\n    plot_type: Optional[str] = None,\n    dimensions: Optional[List[str]] = None,\n    metrics: Optional[List[str]] = None,\n    color_by: Optional[str] = None,\n    title: Optional[str] = None,\n    stacked: bool = False,\n    figsize: Optional[Tuple[int, int]] = None,\n    orientation: str = 'vertical',\n    palette: Optional[str] = None,\n    sort_values: bool = False,\n    sort_ascending: Union[bool, List[bool]] = True,\n    sort_by: Optional[Union[str, List[str], Tuple[str, ...], List[Tuple[str, ...]]]] = None,\n    pivot: Optional[Union[str, List[str]]] = None,\n    limit: Optional[int] = None,\n    formatter: Optional[Dict[str, str]] = None,\n    legend_position: Optional[str] = None,\n    annotations: Optional[Dict[str, Any]] = None,\n    custom_options: Optional[Dict[str, Any]] = None,\n    hide_index: bool = False,\n    row_color_condition: Optional[str] = None,\n    row_colors: Optional[Dict[str, str]] = None,\n    set_as_default: bool = True,\n) -&gt; None\n</code></pre> <p>Persist a plot configuration for a query. If <code>plot_type</code> is None, an appropriate type is suggested. The stored configuration keeps the original user-provided <code>dimensions</code> and <code>metrics</code> under <code>_input_dimensions</code> and <code>_input_metrics</code> so plots automatically refresh if the query is redefined.</p> <p>Notes: - If <code>plot_name</code> is omitted a descriptive auto-generated name is used (e.g. \"bar plot\"). - The function registers a dependency (query -&gt; plot) so plot configs are refreshed when the query changes. - If <code>dimensions</code> or <code>metrics</code> are omitted they default to the query's definition (metrics includes derived metrics).</p>"},{"location":"api/plotting_system/#get_plots","title":"get_plots","text":"<pre><code>get_plots(query_name: str) -&gt; Dict[str, Dict[str, Any]]\n</code></pre> <p>Return all plot configurations for a query.</p>"},{"location":"api/plotting_system/#get_plot_config","title":"get_plot_config","text":"<pre><code>get_plot_config(query_name: str, plot_name: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Return the configuration for a specific plot (default if None).</p> <p>Note: the implementation raises a <code>ValueError</code> if the query is undefined or has no plot configurations.</p>"},{"location":"api/plotting_system/#set_default_plot","title":"set_default_plot","text":"<pre><code>set_default_plot(query_name: str, plot_name: str) -&gt; None\n</code></pre> <p>Set the default plot for a query.</p>"},{"location":"api/plotting_system/#get_default_plot","title":"get_default_plot","text":"<pre><code>get_default_plot(query_name: str) -&gt; Optional[str]\n</code></pre> <p>Return the default plot name.</p>"},{"location":"api/plotting_system/#list_plots","title":"list_plots","text":"<pre><code>list_plots(query_name: str) -&gt; List[str]\n</code></pre> <p>List all plot names for a query.</p>"},{"location":"api/plotting_system/#suggest_plot_types","title":"suggest_plot_types","text":"<pre><code>suggest_plot_types(query_name: str) -&gt; List[Dict[str, Any]]\n</code></pre> <p>Suggest suitable plot types given the query\u2019s (dimensions, metrics).</p>"},{"location":"api/plotting_system/#set_plot_renderer","title":"set_plot_renderer","text":"<pre><code>set_plot_renderer(renderer: PlotRenderer) -&gt; None\n</code></pre> <p>Set the default renderer.</p>"},{"location":"api/plotting_system/#set_config_resolver","title":"set_config_resolver","text":"<pre><code>set_config_resolver(resolver: PlotConfigResolver) -&gt; None\n</code></pre> <p>Set the suggestion/validation resolver.</p>"},{"location":"api/plotting_system/#plot","title":"plot","text":"<pre><code>plot(\n    query_name: Optional[str] = None,\n    query_options: Optional[Dict[str, Any]] = None,\n    plot_name: Optional[Union[str, List[str]]] = None,\n    plot_type: Optional[str] = None,\n    renderer: Optional[PlotRenderer] = None,\n    **kwargs,\n) -&gt; Any\n</code></pre> <p>Render a plot for a stored or ad\u2011hoc query. If plot_name is a list, returns multiple outputs using a shared query result.</p> <p>Notes:</p> <ul> <li> <p>When <code>plot_name</code> is a list the method returns a list of rendered outputs (one per plot) and enforces <code>query_options is None</code> (it reuses the stored query definition).</p> </li> <li> <p>The default renderer is <code>MatplotlibRenderer()</code> but you can override it per-call via the <code>renderer</code> parameter or set a global default using <code>set_plot_renderer()</code>.</p> </li> </ul>"},{"location":"api/plotting_system/#delete_plot","title":"delete_plot","text":"<pre><code>delete_plot(query_name: str, plot_name: str) -&gt; None\n</code></pre> <p>Remove a plot configuration and its dependency edges.</p> <p>Notes:</p> <ul> <li> <p>Dimensions/metrics default to the query definition if omitted.</p> </li> <li> <p>When a query is redefined, linked plot configs auto-refresh to reflect updated dims/mets.</p> </li> <li> <p>Plot configurations are persisted via the Model Catalog alongside metrics, queries, and transformers.</p> </li> </ul>"},{"location":"api/query_methods/","title":"Query Methods","text":""},{"location":"api/query_methods/#query","title":"query","text":"<pre><code>query(\n    query_name: Optional[str] = None,\n    options: Optional[Dict[str, Any]] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Execute a stored query by name, or pass options to run an ad\u2011hoc query (without persisting it).</p> <p>When options is provided, accepted keys mirror define_query parameters: dimensions, metrics, derived_metrics, having, sort, drop_null_dimensions, drop_null_metric_results.</p> <p>Note: the implementation also accepts an internal flag <code>_retrieve_query_name: bool</code> (used when plotting or creating ad-hoc queries) which, if True, returns a tuple (generated_query_name, DataFrame) instead of only the DataFrame. This parameter is primarily for internal use but exists on the public method signature.</p>"},{"location":"api/query_methods/#add_functions","title":"add_functions","text":"<pre><code>add_functions(**kwargs) -&gt; None\n</code></pre> <p>Add variables/functions available to expressions and <code>DataFrame.query</code> via <code>@name</code>.</p> <p>Notes:</p> <ul> <li> <p>Use <code>cube.add_functions(my_udf=my_udf, normalize=normalize)</code> to register.</p> </li> <li> <p>Access/replace the whole registry via <code>cube.function_registry</code> (a dict-like object).</p> </li> <li> <p>Only importable, top-level functions/classes/modules are persisted across <code>cube.save_as_pickle()</code>/<code>Hypercube.load_pickle()</code>; lambdas and locally defined callables are skipped.</p> </li> <li> <p>If a query references a function that wasn\u2019t restored after load, execution raises; re-register the function first.</p> </li> </ul>"},{"location":"api/query_methods/#dimensions","title":"dimensions","text":"<pre><code>dimensions(\n    columns_to_fetch: List[str],\n    retrieve_keys: bool = False,\n    context_state_name: str = 'Default',\n    query_filters: Optional[Dict[str, Any]] = None,\n) -&gt; pd.DataFrame\n</code></pre> <p>Return unique combinations for requested dimensions, optionally including link keys.</p>"},{"location":"api/query_methods/#dimension","title":"dimension","text":"<pre><code>dimension(\n    dimension: str,\n    context_state_name: str = 'Default',\n    query_filters: Optional[Dict[str, Any]] = None,\n) -&gt; List[str]\n</code></pre> <p>Return distinct values for a single dimension.</p>"},{"location":"api/query_methods/#reset_specs","title":"reset_specs","text":"<pre><code>reset_specs() -&gt; None\n</code></pre> <p>Clear all defined analytics specifications in the hypercube: base metrics, derived metrics, queries, plotting components, transformation components, and reset the function registry to defaults. This also clears the dependency index used to track sources -&gt; dependents. Use this when you want a clean slate for analytics definitions.</p>"},{"location":"api/query_methods/#function_registry-property","title":"function_registry (property)","text":"<pre><code>function_registry -&gt; Any\n# setter accepts: FunctionRegistry | dict | None | mapping-convertible\n</code></pre> <p>Central registry of functions/modules available to metric and derived-metric expressions and to DataFrame.query (via <code>@name</code>).</p> <p>Setting the <code>function_registry</code> accepts: - a <code>FunctionRegistry</code> instance, - a <code>dict</code> of name-&gt;callable, - <code>None</code> to reset to defaults, - or any mapping convertible to <code>dict</code>.</p> <p>The registry is used during evaluation and also controls which callables are persisted when pickling the hypercube (only importable top-level callables are storable).</p>"},{"location":"api/transformation/","title":"Transformation API","text":"<p>Manage query transformations (single-step dataframe augmentations) via registered transformers.</p>"},{"location":"api/transformation/#register_transformer","title":"register_transformer","text":"<pre><code>register_transformer(name: str, transformer: Union[Transformer, Callable[..., pd.DataFrame]]) -&gt; None\n</code></pre> <p>Register an transformer by name. Notes:</p> <ul> <li> <p>The implementation registers a set of default transformers (if available) during initialization. You can override or add new transformers via this method.</p> </li> <li> <p><code>transformer</code> may be an instance implementing the <code>Transformer</code> interface or a plain callable that accepts a DataFrame and returns a DataFrame.</p> </li> </ul>"},{"location":"api/transformation/#define_transformation","title":"define_transformation","text":"<pre><code>define_transformation(\n  query_name: str,\n  transformer: str,\n  params: Optional[Dict[str, Any]] = None,\n) -&gt; None\n</code></pre> <p>Create or update an transformation for a query. Each transformer can be configured at most once per query. Notes: - Defining a transformation registers a dependency in the cube's dependency index so downstream refreshes can be tracked.</p>"},{"location":"api/transformation/#list_transformations","title":"list_transformations","text":"<pre><code>list_transformations(query_name: str) -&gt; List[str]\n</code></pre> <p>List configured transformer names for the query.</p>"},{"location":"api/transformation/#get_transformation_config","title":"get_transformation_config","text":"<pre><code>get_transformation_config(query_name: str, transformer: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Return stored parameters for a transformer in a query. Notes:</p> <ul> <li> <p>If <code>overwrite=True</code> the transform result replaces the provided DataFrame; otherwise only newly added columns are joined into the original result.</p> </li> <li> <p>If <code>copy_input=True</code> the input DataFrame is copied before calling the transformer; otherwise the transformer may mutate the provided DataFrame.</p> </li> <li> <p>The method raises if the named transformer is not registered.</p> </li> </ul>"},{"location":"api/transformation/#delete_transformation","title":"delete_transformation","text":"<pre><code>delete_transformation(query_name: str, transformer: str) -&gt; None\n</code></pre> <p>Delete an transformation configuration from a query.</p>"},{"location":"api/transformation/#transform","title":"transform","text":"<pre><code>transform(\n  df: pd.DataFrame,\n  transformer: Optional[str] = None,\n  params: Optional[Dict[str, Any]] = None,\n  *,\n  overwrite: bool = False,\n  copy_input: bool = False,\n  **overrides,\n) -&gt; pd.DataFrame\n</code></pre> <p>Run a single-step transformation using a registered transformer on the given DataFrame.</p>"},{"location":"concepts/context-state/","title":"Context State","text":"<p>You can think of it as a separate filtering environment.</p> <p>There are two main context states:</p> <ul> <li> <p>'Unfiltered': This is created at the hypercube initialization. It has all the relationships that exists accross all the tables that conform the DAG when there is no filter applied. It is a special internal use state, and is not allowed to filter on it.</p> </li> <li> <p>'Default': When initializing the hypercube on memory, we take a copy of the unfiltered state. Then by default the filters and queries will use this context state, but you can change that..</p> </li> </ul> <p>More states (?)</p> <p>You can also create more context states, as many as you want. They allow you to create multiple independent filtering contexts within the same app so you can compare, isolate, simulate different scenarios side by side or use to create more advanced filtering scenarios.</p> <p>Creating new context states:</p> <pre><code># Create a new filtering environment\ncube.set_context_state('New Analysis')\n\n# Apply filters specific to this context\ncube.filter(\n    {'date': ['2024-10-01', '2024-11-01', '2024-12-01']}, \n    context_state_name='New Analysis'\n)\n\n# Define metrics using the specific context state\ncube.define_metric(\n    name='New Analysis Revenue',\n    expression='[qty] * [price]', \n    aggregation='sum',\n    context_state_name='New Analysis'  # This metric uses the New Analysis context\n)\n\n# Define regular metrics (using Default context)\ncube.define_metric(\n    name='Regular Revenue',\n    expression='[qty] * [price]', \n    aggregation='sum'\n    # No context_state_name means it uses 'Default'\n)\n\n# Define a query that will use both metrics\ncube.define_query(\n    name=\"revenue_comparison\",\n    dimensions={'region'},\n    metrics=['Regular Revenue', 'New Analysis Revenue']\n    # Queries don't have context_state_name parameter\n)\n\n# Execute the query - it will show revenue from both context states\ncomparison_results = cube.query(\"revenue_comparison\")\n</code></pre> <p>Bear in mind that when creating a new context state, a copy of the 'Unfiltered' context state holding all the relationships will be made and stored on memory.</p>"},{"location":"concepts/dimensions/","title":"Dimensions","text":"<p>Dimensions are used to slice and group your data. Think of them as the \"by what\" in your analysis\u2014region, product category, time period, customer segment, and so on.</p> <p>In Cube Alchemy, any column from any table in your hypercube can serve as a dimension.</p> <p>How dimensions work:</p> <ul> <li> <p>Available everywhere: Once your DataFrames are connected in the hypercube, dimensions from any table can be used in any query</p> </li> <li> <p>Auto-joining: When you query dimensions from different tables, the hypercube automatically traverses the relationships to bring them together</p> </li> <li> <p>Multi-hop: You can combine dimensions that are several \"hops\" apart in your data model\u2014like querying by both customer region and product category even if they're in completely separate tables</p> </li> </ul> <p>Getting dimension values:</p> <pre><code># Get combinations of region and category\ncube.dimensions(['region', 'category'])\n\n# Or you can use the query method, but you first need to define the query without metrics\ncube.define_query(\n    name=\"dimension_combinations\",\n    dimensions=set(['region', 'category'])\n)\ncube.query('dimension_combinations')\n</code></pre>"},{"location":"concepts/filters/","title":"Understanding Filters","text":"<p>Filters in Cube Alchemy let you focus your analysis on specific slices of data without modifying your underlying queries.</p>"},{"location":"concepts/filters/#how-filtering-works","title":"How Filtering Works","text":"<p>When you apply a filter to a context state, all queries and metrics operate only on the filtered data until you change or remove the filter:</p> <ol> <li> <p>Apply a filter to select specific data values</p> </li> <li> <p>Execute queries to analyze just the filtered data</p> </li> <li> <p>Modify or remove filters when you want to change focus</p> </li> </ol> <pre><code># Define metrics first\ncube.define_metric(name='Revenue', expression='[qty] * [price]', aggregation='sum')\n\n# Now all queries only see North and West data\ncube.define_query(\n    name=\"sales\",\n    dimensions={'category'},\n    metrics=['Revenue']\n)\n\n# Focus on specific regions\ncube.filter({'region': ['North', 'West']})\n\ncube.query(\"sales\")\n</code></pre> <p>It works seamlessly across all your connected tables. When you filter by customer region, it automatically affects sales data, product data, and anything else connected through your data model relationships.</p>"},{"location":"concepts/filters/#basic-filtering-operations","title":"Basic Filtering Operations","text":""},{"location":"concepts/filters/#apply-filters","title":"Apply Filters","text":"<p>Filters are defined as dictionaries where keys are dimension names and values are lists of allowed values:</p> <pre><code># Single dimension, multiple values\ncube.filter({'region': ['North', 'West', 'South']})\n\n# Multiple dimensions at once\ncube.filter({\n    'region': ['North', 'West'], \n    'product_category': ['Electronics'],\n    'customer_segment': ['Enterprise', 'SMB']\n})\n</code></pre>"},{"location":"concepts/filters/#remove-specific-filters","title":"Remove Specific Filters","text":"<p>Remove filters from specific dimensions while keeping others intact:</p> <pre><code># Remove filter on region only (keep other filters)\ncube.remove_filter(['region'])\n\n# Remove multiple filters at once\ncube.remove_filter(['region', 'product_category'])\n</code></pre>"},{"location":"concepts/filters/#reset-all-filters","title":"Reset All Filters","text":"<p>Clear all filters to return to the full dataset:</p> <pre><code># Remove all filters (back to unfiltered data)\ncube.reset_filters(direction='all')\n</code></pre>"},{"location":"concepts/filters/#view-current-filters","title":"View Current Filters","text":"<p>Check which filters are currently active:</p> <pre><code># Get dictionary of active filters\ncurrent_filters = cube.get_filters()\nprint(current_filters)\n# Output: {'region': ['North', 'West'], 'category': ['Electronics']}\n</code></pre>"},{"location":"concepts/filters/#filter-history-and-undoredo","title":"Filter History and Undo/Redo","text":"<p>Cube Alchemy maintains your filtering history, allowing you to navigate through previous filter states:</p> <pre><code># Undo last filter operation (step backward)\ncube.reset_filters(direction='backward')\n\n# Redo previously undone filter (step forward)\ncube.reset_filters(direction='forward')\n\n# Clear all filters and history\ncube.reset_filters(direction='all')\n\n# See which dimensions currently have active filters\nfiltered_dims = cube.get_filtered_dimensions()\n</code></pre>"},{"location":"concepts/filters/#advanced-filtering-techniques","title":"Advanced Filtering Techniques","text":""},{"location":"concepts/filters/#per-metric-filters","title":"Per-Metric Filters","text":"<p>While filters apply globally to your hypercube's context state, individual metrics can have their own additional filters:</p> <pre><code># Global filter: only Electronics category\ncube.filter({'category': ['Electronics']})\n\n# Define metric with additional filter: only Premium-tier electronics\ncube.define_metric(\n    name='Premium Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    metric_filters={'price_tier': ['Premium']}  # Additional filter just for this metric\n)\n</code></pre>"},{"location":"concepts/filters/#unfiltered-and-default-context-states","title":"'Unfiltered' and 'Default' Context States","text":"<p>Cube Alchemy maintains two special context states called 'Unfiltered' and 'Default'.</p> <pre><code>- 'Unfiltered' always contains your full dataset in the context and cannot be updated.\n\n- 'Default' is used as default (as you might have guessed).\n</code></pre> <p>And 'Unfiltered' can also be used on the metric context state: </p> <pre><code># Define standard revenue metric in the default (filtered) context\ncube.define_metric(\n    name='Filtered Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum'\n    #, context_state_name='Default'\n)\n\n# Apply some filters to the default context\ncube.filter({'region': ['North', 'South']}) # Applies on 'Default'\n\n# Define a metric using the special Unfiltered context\ncube.define_metric(\n    name='Total Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    context_state_name='Unfiltered'  # Always uses the full dataset\n)\n\n# Define a query that compares filtered and unfiltered metrics\ncube.define_query(\n    name=\"revenue_comparison\",\n    dimensions={'product_category'},\n    metrics=['Filtered Revenue', 'Total Revenue']\n)\n\n# The result will show revenue for North and South regions alongside\n# the total revenue across all regions for each product category\nresult = cube.query(\"revenue_comparison\")\n</code></pre>"},{"location":"concepts/hypercube/","title":"Understanding the Hypercube","text":"<p>The hypercube is a powerful abstraction for analyzing data from multiple perspectives. Think of a simple cube with three dimensions: length, width, and height. A hypercube extends this concept to an arbitrary number of dimensions, allowing you to model complex business scenarios.</p> <p>Each axis of the hypercube represents a key dimension of your data, such as:</p> <ul> <li>Time (e.g., year, month, day)</li> <li>Geography (e.g., region, country, city)</li> <li>Product (e.g., category, brand, item)</li> <li>Customer (e.g., segment, demographics)</li> </ul> <p>The hypercube enables you to calculate metrics (like sales, revenue, or user counts) across any combination of these dimensions, ensuring your calculations are always consistent and reusable. For example, you can analyze:</p> <ul> <li> <p>Revenue by Month</p> </li> <li> <p>Active users by Month and Product Category</p> </li> <li> <p>Margin by Month, Product Category, and Region</p> </li> </ul>"},{"location":"concepts/hypercube/#data-model-as-a-graph","title":"Data Model as a Graph","text":"<p>This is how Cube Alchemy Builds an Hypercube. The underlying structure is a Connected Undirected Acyclic Graph where:</p> <ul> <li>Nodes: Your DataFrames.</li> <li>Edges: Shared columns link tables. **</li> </ul> <p>It might not look like a tree, but under the hood, the hypercube is basically a tree in disguise \u2014 no loops, just clean branches connecting your data. See: Tree (graph theory).</p> <ul> <li> <p>Effortless Queries: The hypercube handles the relationships for you. When you request a metric across certain dimensions, the hypercube automatically traverses these connections to gather the necessary data.</p> </li> <li> <p>Consistency: Define your metrics and queries once; they will be stateful and can be used reliably across the entire analysis journey.</p> </li> </ul> <p>Notes:</p> <p>** Practically, when three or more tables share the same column name, linking them directly would create cycles. Cube Alchemy prevents this by:</p> <ul> <li> <p>creating an extra node (a link table) with the distinct shared values,</p> </li> <li> <p>replacing the original column in each table with an auto\u2011numbered key,</p> </li> <li> <p>keeping the original column name only in the link table,</p> </li> <li> <p>using those key columns solely to connect tables (they\u2019re internal and hidden for users).</p> </li> </ul> <p>Avoid Circular Dependencies</p> <p>For the hypercube to function correctly, your data model must be a Acyclic Graph. This means you must avoid cyclic relationships (or circular references). Such cycles create ambiguous join paths that break the logic of data traversal and aggregations.</p> <p>With proper modeling (e.g., bridge tables, role-playing/conformed dimensions, etc) cycles can be addressed. Trees keep results consistent and avoid the ad\u2011hoc rules general graphs require.</p>"},{"location":"concepts/metrics/","title":"Understanding Metrics","text":"<p>A metric is a calculated measure that aggregates data in some meaningful way. Think revenue, profit margin, average order size, customer count\u2014basically any aggregated value that helps you analyze your data.</p> <p>In Cube Alchemy, metrics are defined once and stored within the cube object for later use:</p> <ol> <li> <p>You define a metric using <code>cube.define_metric()</code> providing at least a name, expression, and aggregation method</p> </li> <li> <p>The metric is stored in the cube object</p> </li> <li> <p>Later, you reference metrics by name when defining queries</p> </li> </ol>"},{"location":"concepts/metrics/#building-a-metric","title":"Building a Metric","text":"<p>Every metric needs three essential components:</p> <ol> <li> <p>Name: A clear, descriptive label for the metric (e.g., 'Revenue', 'Customer Count')</p> </li> <li> <p>Expression: The calculation formula, using dimension references inside square brackets (e.g., <code>[qty] * [price]</code>)</p> </li> <li> <p>Aggregation: How to combine values\u2014standard methods like <code>sum</code>, <code>mean</code>, <code>count</code>, or custom functions</p> </li> </ol> <pre><code># Step 1: Define your metrics\ncube.define_metric(\n    name='Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum'\n)\n\ncube.define_metric(\n    name='Average Order Value', \n    expression='[price]',\n    aggregation='mean'\n)\n\ncube.define_metric(\n    name='Number of Orders',\n    expression='[order_id]',\n    aggregation=lambda x: x.nunique()\n)\n\n# Step 2: Define a query that uses these metrics\ncube.define_query(\n    name=\"sales_performance\",\n    dimensions={'region', 'product_category'},\n    metrics=['Revenue', 'Average Order Value', 'Number of Orders']\n)\n\n# Step 3: Execute the query by referencing its name\nresult = cube.query(\"sales_performance\")\n\n# The metrics are calculated and returned as columns in the result DataFrame\n</code></pre>"},{"location":"concepts/metrics/#syntax-rules","title":"Syntax Rules","text":"<ul> <li> <p>Column References: Columns in metric expressions must be enclosed in square brackets: <code>[qty]</code>, <code>[price]</code>, <code>[cost]</code>, etc.</p> </li> <li> <p>Aggregation Methods: The <code>aggregation</code> parameter accepts:</p> <ul> <li> <p>Pandas group by strings: <code>'sum'</code>, <code>'mean'</code>, <code>'count'</code>, <code>'min'</code>, <code>'max'</code>, etc.</p> </li> <li> <p>You can extend the pandas group by strings by registering custom functions, which will aggregate over the resultin pandas series. See below.</p> </li> <li> <p>Custom callable functions: <code>lambda x: x.quantile(0.95)</code> or any function that accepts a pandas Series.</p> </li> </ul> </li> </ul> <p>NOTE: Avoid passing custom callabes if you plan to work with YAML (or other source) catalog to persist these definitions. After registering it you can use it.</p>"},{"location":"concepts/metrics/#derived-metrics","title":"Derived Metrics","text":"<p>Derived metrics are calculated after aggregation has already occurred. While regular metrics aggregate over dimensions, derived metrics work with already aggregated results, letting you create ratios, percentages, and other derivative calculations.</p> <pre><code># First define the metrics needed for the computation\ncube.define_metric(\n    name='Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum'\n)\n\ncube.define_metric(\n    name='Cost',\n    expression='[qty] * [unit_cost]',\n    aggregation='sum'\n)\n\n# Then define a derived metric that uses them\ncube.define_derived_metric(\n    name='Profit Margin %',\n    expression='([Revenue] - [Cost]) / [Revenue] * 100'\n)\n\n# Use both regular and derived metrics in queries\ncube.define_query(\n    name=\"profitability_analysis\",\n    dimensions={'product_category', 'region'},\n    derived_metrics=['Profit Margin %']\n)\n\n# Execute the query\nresult = cube.query(\"profitability_analysis\")\n</code></pre> <p>The workflow is:</p> <ol> <li> <p>Define regular metrics that perform aggregation</p> </li> <li> <p>Define derived metrics that reference those aggregated metrics</p> </li> <li> <p>Include them in your queries (derived metrics are passed separately)</p> </li> </ol>"},{"location":"concepts/metrics/#advanced-features","title":"Advanced Features","text":"<p>For more sophisticated analysis, metrics support several powerful options:</p> <ul> <li> <p>Different context states: Calculate metrics in different filtering environments</p> </li> <li> <p>Metric filters: Apply specific filters only for a particular metric</p> </li> <li> <p>Row conditions: Pre-filter rows before calculating the metric</p> </li> <li> <p>Ignore Dimensions: Control dimensional aggregation behavior</p> </li> <li> <p>Ignore Context Filters (per metric): Let a metric ignore all or some of the active context filters</p> </li> <li> <p>Nested Aggregations: two-step aggregations (see below). </p> </li> <li> <p>Custom functions: Use your own Python functions for complex logic</p> </li> </ul> <p>Each of these options allows you to create highly specialized metrics that can answer specific more sophisticated questions.</p> <pre><code># Only count high-value orders\ncube.define_metric(\n    name='High Value Orders',\n    expression='[order_id]',\n    aggregation='count',\n    row_condition_expression='[price] &gt; 100'\n)\n\n# Revenue only from specific regions (metric-level filter)\ncube.define_metric(\n    name='Regional Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    metric_filters={'region': ['North', 'West']}\n)\n\n# Ignore all context filters only for this metric (computed over the Unfiltered state)\ncube.define_metric(\n    name='Revenue (All Context)',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    ignore_context_filters=True\n)\n\n# Ignore only some context filters (e.g., ignore the country filter, respect the rest)\ncube.define_metric(\n    name='Revenue (Ignoring Country Filter)',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    ignore_context_filters=['country']\n)\n\n# Create a new Context State\ncube.set_context_state('My New Context')\n\n# Apply different filters to it\ncube.filter(\n    {'date': ['2024-10-01', '2024-11-01', '2024-12-01']},\n    context_state_name='My New Context'\n)\n\n# Define a metric using the new context\ncube.define_metric(\n    name='High Value Orders',\n    expression='[order_id]',\n    aggregation='count',\n    context_state_name='My New Context'\n)\n\n# Ignore_dimensions from aggregation\ncube.define_metric(\n    name='Total Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    ignore_dimensions=True  # Ignores all dimensions, returns same value for all rows\n)\n\n# Ignore specific dimensions from aggregation\ncube.define_metric(\n    name='Country Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    ignore_dimensions=['city', 'product', 'date']  # Ignore these dimensions when aggregating\n)\n\n# Define a query with these metrics\ncube.define_query(\n    name=\"advanced_analysis\",\n    dimensions=set(my_query_dimensions),\n    metrics=['High Value Orders', 'Regional Revenue', 'High Value Orders']\n)\n\n# Execute the query\nresult = cube.query(\"advanced_analysis\")\n</code></pre>"},{"location":"concepts/metrics/#ignore-context-filters","title":"Ignore Context Filters","text":"<p>When a metric specifies <code>ignore_context_filters</code>:</p> <ul> <li> <p>If set to <code>True</code>, the metric is evaluated against the Unfiltered context (equivalent to using <code>context_state_name='Unfiltered'</code> for that metric). Any <code>metric_filters</code> on the metric still apply.</p> </li> <li> <p>If set to a list of dimensions, the metric is evaluated against its context state with those specific filters removed; then the metric's own <code>metric_filters</code> (if any) are applied on top.</p> </li> </ul>"},{"location":"concepts/metrics/#nested-aggregation","title":"Nested aggregation","text":"<p>Nested aggregation runs the metric in two stages so you can build staged calculations (for example: \"average of per-product totals\"). Use this when you need to aggregate at a finer grouping first, then combine those intermediate results into the final value.</p> <p>How it works:</p> <ol> <li> <p>Stage 1: compute the metric by grouping on the metric's effective dimensions plus the <code>nested.dimensions</code>. This produces one value per group defined by those combined dimensions.</p> </li> <li> <p>Stage 2: aggregate the stage-1 results by the metric's effective dimensions (usually the dimensions in your query). The metric's main <code>aggregation</code> setting is used in this final step.</p> </li> </ol> <p>Key points:</p> <ul> <li><code>nested.aggregation</code> is the aggregation used in stage 1; the metric's <code>aggregation</code> is applied in stage 2.</li> <li><code>nested.dimensions</code> only change how the first stage groups the rows.</li> <li>The metric's parameter <code>ignore_dimensions</code> applies to both stages and defines the metric's effective dimensions.</li> </ul> <p>Example:</p> <pre><code>cube.define_metric(\n    name='Avg Product Revenue',\n    expression='[qty] * [price]',\n    aggregation='mean',                # final aggregation over product sums\n    nested={'dimensions': ['product'], 'aggregation': 'sum'},\n    ignore_dimensions=['store']         # compute at a higher level and broadcast to store rows\n)\n</code></pre> <p>Explanation: this computes total revenue per product first (stage 1, <code>sum</code> by product), then takes the average of those product totals across the query dimensions (stage 2, <code>mean</code>).</p>"},{"location":"concepts/metrics/#custom-functions","title":"Custom Functions","text":"<p>When your analysis requires logic that goes beyond basic arithmetic, you can register and use custom Python functions:</p> <ol> <li> <p>Define a Python function that performs your specialized calculation</p> </li> <li> <p>Register the function with your cube using <code>cube.add_functions()</code></p> </li> <li> <p>Reference the function in your metric expressions using the <code>@function_name</code> syntax (Pandas and Numpy are already registered as pd and np).</p> </li> </ol> <p>This powerful feature allows you to implement virtually any calculation logic while keeping your metric definitions clean and readable.</p> <pre><code>import numpy as np\n\n# Define and register a safe division function\ndef safe_division(numerator, denominator, default=0.0):\n    \"\"\"Safely divide two arrays, handling division by zero\"\"\"\n    result = numerator / denominator\n    return result.replace([np.inf, -np.inf], np.nan).fillna(default)\n\n# Register the function with your hypercube\ncube.add_functions(safe_division=safe_division)\n\n# Use it in a metric definition\ncube.define_metric(\n    name='Profit Margin %',\n    expression='@safe_division([revenue] - [cost], [revenue]) * 100',\n    aggregation='mean'\n)\n\n# Another example: categorizing data\ndef categorize_revenue(revenue_values):\n    \"\"\"Categorize revenue into tiers\"\"\"\n    conditions = [\n        revenue_values &lt; 1000,\n        (revenue_values &gt;= 1000) &amp; (revenue_values &lt; 5000),\n        revenue_values &gt;= 5000\n    ]\n    choices = ['Low', 'Medium', 'High']\n    return np.select(conditions, choices, default='Unknown')\n\ncube.add_functions(categorize_revenue=categorize_revenue)\n\n# Use for conditional logic - count how many sales fall into each tier\ncube.define_metric(\n    name='Revenue Tier Count',\n    expression='@categorize_revenue([qty] * [price])',\n    aggregation=lambda x: len(x)\n)\n\n# Or get the most common revenue tier\ncube.define_metric(\n    name='Most Common Revenue Tier',\n    expression='@categorize_revenue([qty] * [price])',\n    aggregation=lambda x: x.value_counts().index[0]\n)\n\n# All pandas and numpy functions are already registered functions and can be used right away by calling pd or np\n\n# for example if you have this metric\ncube.define_metric(\n    name='High Value Orders',\n    expression='[order_id]',\n    aggregation='count',\n    row_condition_expression='[price] &gt; 100'\n)\n\n# it can be defined also using numpy inside the expression\n\ncube.define_metric(\n    name='High Value Orders',\n    expression='@np.where([price] &gt; 100, [order_id], np.nan)',\n    aggregation='count'\n)\n\n# Will retrieve the exact same value\n\n# Advanced handling of missing values with @ functions\n# While the basic fillna parameter fills all metric columns with the same value:\ncube.define_metric(\n    name='Revenue',\n    expression='[qty] * [price]',\n    aggregation='sum',\n    fillna=0  # Fills both [qty] and [price] with 0\n)\n\n# For column-specific NA handling, use @ functions directly in the expression:\ncube.define_metric(\n    name='Revenue with Custom NA Handling',\n    expression='@pd.Series([qty]).fillna(1) * @pd.Series([price]).fillna(0)',\n    aggregation='sum'  # Fills [qty] with 1 and [price] with 0\n)\n\n# Using np.where for conditional NA handling:\ncube.define_metric(\n    name='Revenue with Conditional Defaults',\n    expression='@np.where(@pd.isnull([qty]), 0 , [qty])',\n    aggregation='sum'\n)\n\n# You can inspect available registered functions:\ncube.function_registry\n</code></pre> <p>Note on Function Handling</p> <p>Metric aggregation allows you to pass custom functions directly while expression functions need to be registered on the hypercube first and referenced with <code>@function_name</code>. This difference exists due to their roles in the processing pipeline:</p> <ul> <li> <p>Expression functions operate inside within dataframe rows before aggregation</p> </li> <li> <p>Aggregation functions work outside on the grouped data</p> </li> </ul> <p>So you can in fact pass even lambda functions to the aggregations. Please bear in mind that if not registered, these functions cannot be persisted into model_catalog Sources, for instance you define a metric using a lambda function then &lt; lambda &gt; is stored on your YAML model catalog and on the way back when trying to read it python will not know how to interpret that. So if working with external model catalog is a good practice to register all the functions that are going to be used in your hypercube.</p>"},{"location":"concepts/model_catalog/","title":"Model Catalog","text":"<p>The Model Catalog makes working with your hypercube more efficient by persisting analytics and plotting definitions to external storage. This lets you separate configuration from code and work across sessions.</p> <p>By default, Cube Alchemy uses YAML files to store your model definitions. You can:</p> <ul> <li> <p>Define metrics, queries, and visualizations directly in YAML</p> </li> <li> <p>Load external definitions into a fresh hypercube session</p> </li> <li> <p>Save definitions you've created in code back to YAML</p> </li> </ul> <p>This approach separates your analytical definitions from your processing code, creating a cleaner, more maintainable workflow.</p>"},{"location":"concepts/model_catalog/#what-you-can-persist","title":"What you can persist","text":"<p>Analytics Components</p> <ul> <li> <p>Metrics and derived metrics</p> </li> <li> <p>Transformations</p> </li> <li> <p>Queries</p> </li> </ul> <p>Visualization</p> <ul> <li>Plot configurations</li> </ul>"},{"location":"concepts/model_catalog/#how-it-works","title":"How it works","text":"<ul> <li> <p>Catalog: A fa\u00e7ade that coordinates Sources and Repository operations</p> </li> <li> <p>Sources: Backend connectors that read/write definitions (YAML is the default implementation)</p> </li> <li> <p>Repository: An in-memory store that holds normalized definitions during runtime.</p> </li> </ul> <p>This architecture potentially allows you combine multiple sources while maintaining a consistent interface for your hypercube.</p>"},{"location":"concepts/model_catalog/#basic-usage","title":"Basic usage","text":"<p><pre><code># 1. Attach a YAML file to your hypercube\ncube.set_yaml_model_catalog() # will set \"model_catalog.yaml\" on the working directory\n\n# 2. Load existing definitions from YAML into your cube\n# If you want to start clean before applying the catalog, pass reset_specs=True\ncube.load_from_model_catalog(reset_specs=True)\n\n# 3. Create or modify definitions using the hypercube API. This is useful when creating on-the-fly definitions...\ncube.define_metric(\"revenue\", expression=\"[sales] * [price]\", aggregation=\"sum\")\ncube.define_query(\"sales_by_region\", dimensions=[\"region\"], metrics=[\"revenue\"])\n\n# ...and you want to persist them.\n# 4. Save your definitions back to YAML\ncube.save_to_model_catalog()\n\n# 5. Inspect available definitions by accessing the model_catalog object directly.\nmetrics = cube.model_catalog.list(\"metrics\")\nqueries = cube.model_catalog.list(\"queries\")\n# cube.model_catalog.get(\"metrics\", \"Amount Actual\")\n</code></pre> Note:</p> <p>In API methods, \u201cmodel catalog\u201d refers to the Catalog Source/s for simplicity.</p> <p>The Repository is an internal layer working under the hood that normalizes data so Python can read and write from and to the source/s easily. When you access the model_catalog object directly and list or get from it, bear in mind you are doing it from the Repository, so depending on your where you are in your workflow it might need to be refreshed, either from the Source or the actual Hypercube object:</p> <ul> <li> <p>cube.model_catalog.refresh(kinds, reload_sources, clear_repo) # Same as load_from_model_catalog, but without applying the catalog to the cube</p> </li> <li> <p>cube._model_catalog_pull_from_cube() # Same as save_to_model_catalog, but wihtout saving to the actual source.</p> </li> </ul>"},{"location":"concepts/model_catalog/#yaml-structure","title":"YAML structure","text":"<p>Simply mirrors the respective definition APIs. Plots and Transformers can be nested under their parent query (as shown below) or defined at the top level with a query reference.</p> <pre><code>metrics:\n  revenue:\n    expression: \"[Unit Price] * [Quantity]\"\n    aggregation: sum\n\nderived_metrics:\n  margin_pct:\n    expression: \"margin / revenue\"\n\nqueries:\n  Sales by Month Year :\n    dimensions: [month_year]\n    metrics: [revenue]\n    transformers:\n      moving_average:\n        'on': revenue\n        window: 3\n        sort_by: revenue\n        new_column: revenue_ma_3\n    plots:\n        default_bar:\n          plot_type: line\n          metrics: [revenue_ma_3]\n</code></pre>"},{"location":"concepts/plotting/","title":"Plotting System","text":"<p>Cube Alchemy includes a flexible plotting system that allows you to visualize your query results with any visualization framework of your choice.</p> <p>The plotting system is a framework-agnostic approach to visualizing data from your hypercube. It separates:</p> <ul> <li> <p>What data to show (from your queries)</p> </li> <li> <p>How to configure the visualization (plot configurations)</p> </li> <li> <p>How to render the visualization (through renderer interfaces)</p> </li> </ul>"},{"location":"concepts/plotting/#core-components","title":"Core Components","text":""},{"location":"concepts/plotting/#plot-configurations","title":"Plot Configurations","text":"<p>Plot configurations are stored within queries and define how data should be visualized:</p> <pre><code># Define the query with dimensions and metrics\ncube.define_query(\n    name=\"sales analysis\",\n    dimensions=[\"region\"],\n    metrics=[\"revenue\"],\n)\n\ncube.define_plot(\n    query_name=\"sales analysis\",\n    plot_name=\"simple_bar\",\n    plot_type=\"bar\",\n    #dimensions=[\"region\"],\n    #metrics=[\"revenue\"],\n    orientation=\"horizontal\",\n    sort_values=True,\n    title=\"Revenue by Region\" # if ommited, it will be the query name\n)\n\n# cube.set_plot_renderer(MyRenderer()) # If you want to set up a customer renderer. See Plot Renders below\n\n# Use it with default plot configuration\ncube.plot('sales analysis')\n</code></pre>"},{"location":"concepts/plotting/#plot-configuration-parameters","title":"Plot configuration parameters","text":"<p>The plotting interface accepts a rich set of configuration parameters. Below is the full list used by the current implementation, with short descriptions and default values:</p> <ul> <li>query_name: str<ul> <li>The name of the query this plot config belongs to. (required)</li> </ul> </li> <li>plot_name: Optional[str] = None<ul> <li>A name for the plot configuration. If omitted, a name will be inferred.</li> </ul> </li> <li>plot_type: Optional[str] = None<ul> <li>Type of visualization (e.g., 'bar', 'line', 'area', 'table', 'heatmap'). Renderer-specific handlers may accept custom types.</li> </ul> </li> <li>dimensions: Optional[List[str]] = None<ul> <li>Override the query dimensions for this plot. Provide a list of column names. If not passed all the query dimensions will be used.</li> </ul> </li> <li>metrics: Optional[List[str]] = None<ul> <li>Override the query metrics for this plot. Provide a list of metric names. If not passed all the query metrics and derived metrics will be used.</li> </ul> </li> <li>color_by: Optional[str] = None<ul> <li>Column (dimension or metric) used to color lines/bars/categories.</li> </ul> </li> <li>title: Optional[str] = None<ul> <li>Plot title. If omitted, the query or plot name is used.</li> </ul> </li> <li>stacked: bool = False<ul> <li>For area/bar charts, whether series should be stacked.</li> </ul> </li> <li>figsize: Optional[Tuple[int, int]] = None<ul> <li>Figure size in inches as (width, height). If None, renderer defaults are used.</li> </ul> </li> <li>orientation: str = \"vertical\"<ul> <li>'vertical' or 'horizontal' orientation for bar-like charts.</li> </ul> </li> <li>palette: Optional[str] = None<ul> <li>Color palette name or list accepted by the renderer.</li> </ul> </li> <li>sort_values: bool = False<ul> <li>Whether to sort rows before plotting.</li> </ul> </li> <li>sort_ascending: Union[bool, List[bool]] = True<ul> <li>Sort direction(s). Can be a single bool or a list aligned with <code>sort_by</code>.</li> </ul> </li> <li>sort_by: Optional[Union[str, List[str], Tuple[str, ...], List[Tuple[str, ...]]]] = None<ul> <li>Column(s) to sort by. Accepts a string, list of strings, or tuple(s) for composite sort keys.</li> </ul> </li> <li>pivot: Optional[Union[str, List[str]]] = None<ul> <li>Pivot the data on the given column(s) before rendering (e.g., create a wide table from long data).</li> </ul> </li> <li>limit: Optional[int] = None<ul> <li>Limit the number of rows (e.g., top-N) after sorting.</li> </ul> </li> <li>formatter: Optional[Dict[str, str]] = None<ul> <li>Mapping of column name -&gt; format string (e.g., { 'revenue': '0.2f', 'date': '%Y-%m' }). Used when rendering text values.</li> </ul> </li> <li>legend_position: Optional[str] = None<ul> <li>Position for the legend (e.g., 'best', 'upper right', 'bottom'). Renderer-dependent.</li> </ul> </li> <li>annotations: Optional[Dict[str, Any]] = None<ul> <li>Annotation options passed to the renderer (e.g., {'show_values': True, 'format': '0.0f'}).</li> </ul> </li> <li>custom_options: Optional[Dict[str, Any]] = None<ul> <li>Arbitrary renderer-specific options. Useful for passing through parameters not supported by the core.</li> </ul> </li> <li>hide_index: bool = False<ul> <li>Hide index labels/axis when rendering tables or DataFrame-like outputs.</li> </ul> </li> <li>row_color_condition: Optional[str] = None<ul> <li>A boolean expression (using column names) that determines which rows get special coloring (e.g., '[revenue] &gt; 10000').</li> </ul> </li> <li>row_colors: Optional[Dict[str, str]] = None<ul> <li>Mapping of value -&gt; color for row-level coloring when <code>row_color_condition</code> or categorical coloring is used.</li> </ul> </li> <li>set_as_default: bool = True<ul> <li>If True, mark this plot configuration as the default for the query.</li> </ul> </li> </ul> <p>Example usage:</p> <pre><code>cube.define_plot(\n        query_name='sales analysis',\n        plot_name='top_regions',\n        plot_type='bar',\n        dimensions=['region'],\n        metrics=['revenue'],\n        title='Top Regions by Revenue',\n        stacked=False,\n        figsize=(10, 6),\n        orientation='vertical',\n        sort_values=True,\n        sort_by='revenue',\n        limit=10,\n        palette='tab10',\n        set_as_default=True\n)\n</code></pre> <p>Each query can have multiple plot configurations with one designated as the default.</p>"},{"location":"concepts/plotting/#plot-renderers","title":"Plot Renderers","text":"<p>Renderers are responsible for creating the visualization based on the data and plot configuration. They implement a common interface:</p> <pre><code>from cube_alchemy.plot_renderer import PlotRenderer\n\nclass MyRenderer(PlotRenderer):\n    def render(self, data, plot_config, **kwargs):\n        # Create and return a visualization using the data and configuration\n</code></pre> <p>This design allows Cube Alchemy to integrate with any visualization framework in python including:</p> <ul> <li> <p>Matplotlib (set as Default, thought it could be a brand new one designed)</p> </li> <li> <p>Streamlit</p> </li> <li> <p>Plotly</p> </li> <li> <p>Etc</p> </li> </ul>"},{"location":"concepts/plotting/#registering-a-custom-plot","title":"Registering a Custom Plot","text":"<p>You can add new plot types at runtime. The simplest handler accepts only a DataFrame; the renderer also supports richer signatures.</p> <pre><code>def custom_render(df, dims, mets):\n    print(\"dims:\", dims)\n    print(\"mets:\", mets)\n    print(df)\n\ncube.plot_renderer.register_plot(\"custom\", custom_render) ## This will work on the default Matplotlib Renderer, it adds a new rendering function to it which can be called for all plot_types = custom\n\ncube.plot('Your Query', plot_type=\"custom\")\n\n## Or use a brand new renderer that knows how to render the plot_type = custom\nmy_renderer = CustomRenderer()\ncube.plot('Your Query', renderer = my_renderer, plot_type=\"custom\")\n\n# Note: register_plot() is not part of PlotRenderer (ABC) but a special method for the Default MatplotlibRenderer installed with the Hypercube. If you use custom a renderer it will not be there.\n</code></pre> <p>Notes:</p> <ul> <li> <p>Registration is per Python run; call it during startup or from a module that\u2019s imported early.</p> </li> <li> <p>The same <code>plot_type</code> name will override an existing handler.</p> </li> </ul>"},{"location":"concepts/plotting/#why-a-separate-interface","title":"Why a Separate Interface?","text":"<p>The plotting layer is separate from the core so you can:</p> <ul> <li>Keep data and visualization concerns independent</li> <li>Swap visualization frameworks without touching core logic</li> <li>Evolve as libraries change without core refactors</li> <li>Skip extra dependencies when visualization isn't needed</li> </ul> <p>You can always plot DataFrames directly. Using the plotting interface (renderers and plot configurations) keeps things organized, improves readability and maintainability, and makes updates or framework switches easier.</p>"},{"location":"concepts/plotting/#default-and-custom-plot-renderers","title":"Default and Custom Plot Renderers","text":"<p>Cube Alchemy includes a built-in Matplotlib renderer that is used by default. It works well in Jupyter notebooks and can also render charts inside Streamlit apps.</p> <p>When the plotting system calls a renderer it provides the prepared DataFrame and the complete plot configuration. The renderer decides which configuration fields to apply (for example: title, figsize, palette, annotations, etc.). Custom renderers should accept the data and config and use the keys they need; any unused keys can be ignored.</p> <p>A Streamlit renderer is included as a convenience example. Rendering is intentionally separated from the core: renderers can be extended or replaced without changing core functionality Ideally all but the default renderer will live on it's own project/repo in the future.</p>"},{"location":"concepts/queries/","title":"Understanding Queries","text":"<p>Queries in Cube Alchemy bring together metrics and dimensions to answer specific questions.</p> <p>A query consists of three key components:</p> <ul> <li> <p>Query name: A unique identifier for referencing the query</p> </li> <li> <p>Dimensions: Columns to group by (the \"by what\" in your analysis)</p> </li> <li> <p>Metrics: Measures to calculate (the \"what\" in your analysis)</p> </li> </ul> <pre><code># Define metrics\ncube.define_metric(name='Revenue', expression='[qty] * [price]', aggregation='sum')\ncube.define_metric(name='Order Count', expression='[order_id]', aggregation='count')\n\n# Define a query by region and product category\ncube.define_query(\n    name=\"regional_sales\",\n    dimensions={'region', 'category'},\n    metrics=['Revenue', 'Order Count']\n)\n\n# Execute the query\nresult = cube.query(\"regional_sales\")\n</code></pre>"},{"location":"concepts/queries/#execution-pipeline","title":"Execution Pipeline","text":"<p>When you run a query, Cube Alchemy processes your data in a clear, ordered pipeline:</p> <pre><code>flowchart LR\n  A[\"Apply Context State Filters\"] --&gt; B[\"Fetch Dimensions\"]\n  B --&gt; C[\"Calculate Metrics (Aggregations)\"]\n  C --&gt; D[\"Compute Post-Aggregation Metrics\"]\n  D --&gt; E[\"Apply DataFrame Transformer\"]\n  E --&gt; F[\"Apply HAVING Filter\"]\n  F --&gt; G[\"Apply SORT\"]\n  G --&gt; H[\"Return Final Result\"]</code></pre>"},{"location":"concepts/queries/#query-types","title":"Query Types","text":"<p>Queries must contain either dimensions, metrics, or both (a query cannot lack both).</p>"},{"location":"concepts/queries/#dimension-only-queries","title":"Dimension-Only Queries","text":"<p>When you only need to see what unique dimension combinations:</p> <pre><code># Define a query with only dimensions\ncube.define_query(\n    name=\"dimension_combinations\",\n    dimensions={'region', 'category'}\n)\n\n# Get all unique region/category combinations\ncombinations = cube.query(\"dimension_combinations\")\n</code></pre>"},{"location":"concepts/queries/#metric-only-queries","title":"Metric-Only Queries","text":"<p>When you need to calculate global aggregates:</p> <pre><code># Define metrics\ncube.define_metric(name='Total Revenue', expression='[qty] * [price]', aggregation='sum')\ncube.define_metric(name='Total Orders', expression='[order_id]', aggregation='count')\n\n# Define a query with no dimensions\ncube.define_query(\n    name=\"global_totals\",\n    metrics=['Total Revenue', 'Total Orders']\n)\n\n# Execute the query\nglobal_results = cube.query(\"global_totals\")\n</code></pre>"},{"location":"concepts/queries/#derived-metrics-and-having","title":"Derived Metrics and HAVING","text":"<p>Derived metrics are calculated after base metrics are aggregated. Define them once, then reference by name in queries.</p> <pre><code># Define base metrics\ncube.define_metric(name='Cost',   expression='[cost]',           aggregation='sum')\ncube.define_metric(name='Margin', expression='[qty] * [price] - [cost]', aggregation='sum')\n\n# Define a derived metric (post-aggregation)\ncube.define_derived_metric(\n    name='Margin %',\n    expression='[Margin] / [Cost] * 100',\n    fillna=0\n)\n\n# Use derived metric by name and add a HAVING filter\ncube.define_query(\n    name='margin_by_product',\n    dimensions={'product'},\n    metrics=['Margin', 'Cost'], # If not here, as they are part of 'Margin %' they would be calculated internally but hidden.\n    derived_metrics=['Margin %'],\n    having='[Margin %] &gt;= 20'\n)\n\ndf = cube.query('margin_by_product')\n</code></pre> <p>Notes:</p> <ul> <li>Use [Column] syntax in expressions; functions you register are available as <code>@name</code>.</li> <li>Register functions with <code>cube.add_functions(...)</code>. You can inspect or replace the full registry via <code>cube.function_registry</code>.</li> <li>Derived metrics reference columns present in the aggregated result (metrics and dimensions).</li> </ul>"},{"location":"concepts/queries/#registered-functions","title":"Registered Functions","text":"<p>You can register helper functions for use in metrics. </p> <p>All registered functions operate on 1D data: the first argument is a pandas <code>Series</code> (or NumPy array-like). You may add extra parameters as needed.</p> <ul> <li>Expression functions: return a <code>Series</code> of the same length. Used inside expressions via <code>@name(...)</code>.</li> <li>Filter/HAVING functions: return a boolean <code>Series</code> of the same length. Used in HAVING via <code>@name(...)</code>.</li> <li>Aggregation functions: return a single value per group (e.g., number, text, or list).</li> </ul> <p>Access:</p> <ul> <li> <p>In expressions and HAVING (string evaluation): use <code>@function_name(...)</code>.</p> </li> <li> <p>For aggregations: reference by name (no <code>@</code>), e.g., <code>aggregation='p90'</code>. You can pass a callable, but it will not persist to pickle or YAML; prefer registering the function and using its name.</p> </li> </ul> <p>Examples</p> <pre><code># 1) Expression function: register and use in a metric\ndef double_series(s):\n    return s * 2\n\ncube.add_functions(double=double_series)\ncube.define_metric(\n    name='Double Revenue',\n    expression='@double([qty]) * [price]',\n    aggregation='sum'\n)\n\ncube.define_query(\n    name='double_revenue_by_segment',\n    dimensions=['segment'],\n    metrics=['Double Revenue']\n)\ndf = cube.query('double_revenue_by_segment')\n\n# 2) HAVING function: register and use as a filter\ndef above(s, threshold):\n    return s &gt;= threshold\n\ncube.add_functions(above=above)\ncube.define_metric(name='Revenue', expression='[qty] * [price]', aggregation='sum')\ncube.define_query(\n    name='big_segments',\n    dimensions={'segment'},\n    metrics=['Revenue'],\n    having='@above([Revenue], 1000)'\n)\ndf2 = cube.query('big_segments')\n\n# Tip: to persist functions across save/load, define them in a module and import before registering\n# from udfs import double_series, above\n# cube.add_functions(double=double_series, above=above)\n\n# 3) Aggregation function: register and reference by name (persists if importable)\nimport numpy as np, pandas as pd\ndef p90(x):\n    a = np.asarray(pd.Series(x).dropna())\n    return float(np.percentile(a, 90)) if a.size else float('nan')\n\ncube.add_functions(p90=p90)\ncube.define_metric(name='P90 Revenue', expression='[qty] * [price]', aggregation='p90')\ncube.define_query(\n    name='p90_revenue_by_segment',\n    dimensions={'segment'},\n    metrics=['P90 Revenue']\n)\ndf3 = cube.query('p90_revenue_by_segment')\n</code></pre> <p>Note on Persistence and Function Registry</p> <p>Hypercubes can be saved and loaded with <code>cube.save_as_pickle(...)</code> and <code>Hypercube.load_pickle(...)</code>. The function registry persists by import spec:</p> <ul> <li> <p>Importable top-level objects (modules, functions, classes) are restored.</p> </li> <li> <p>Lambdas, closures, and locally defined callables are not persisted and will be dropped silently.</p> </li> </ul> <p>Example:</p> <pre><code>from cube_alchemy.core.hypercube import Hypercube\nfrom udfs import double  # top-level function in an importable module\n\ncube.add_functions(double=double)        # persisted IF 'udfs' is importable at load time\ncube.add_functions(tmp=lambda x: x + 1)  # not persisted\n\npath = cube.save_as_pickle(pickle_name=\"cube.pkl\")\ncube2 = Hypercube.load_pickle(path)\n\nassert 'double' in cube2.function_registry  # only if 'udfs' is on sys.path\nassert 'tmp' not in cube2.function_registry\n</code></pre> <p>Important:</p> <ul> <li> <p>A function persists only if its defining module is importable in the loading environment (on sys.path, correct module name).</p> </li> <li> <p>If a query or metric references a non-restored function (e.g., a lambda or a function defined only in main), evaluation will raise. Re-register an importable function with the same name before running the query again.</p> </li> </ul>"},{"location":"concepts/queries/#effective-dimensions","title":"Effective dimensions","text":"<p>Effective dimensions are the dimensions that actually determine how a metric is aggregated. They are derived from the query\u2019s dimensions after applying the metric\u2019s <code>ignore_dimensions</code> setting:</p> <ul> <li> <p><code>ignore_dimensions=False</code> (default): effective dimensions = query dimensions.</p> </li> <li> <p><code>ignore_dimensions</code> is a list: effective dimensions = query dimensions minus those listed.</p> </li> <li> <p><code>ignore_dimensions=True</code>: no effective dimensions (grand total).</p> </li> </ul> <p>Nested metrics use the same effective dimensions in both inner and outer steps; <code>nested.dimensions</code> are added only for the inner step.</p>"},{"location":"concepts/queries/#working-with-filters","title":"Working with Filters","text":"<p>Queries automatically respect all active filters on your hypercube, allowing you to:</p> <ol> <li> <p>Define a query once</p> </li> <li> <p>Apply different filters</p> </li> <li> <p>Execute the same query to see different filtered views of your data</p> </li> </ol> <pre><code># Define your query\ncube.define_query(\n    name=\"product_sales\",\n    dimensions={'region', 'product_category'},\n    metrics=['Revenue', 'Order Count']\n)\n\n# Get unfiltered results\nunfiltered_results = cube.query(\"product_sales\")\n\n# Apply filters\ncube.filter({'product_type': ['Electronics', 'Home']})\n\n# Get filtered results using the same query\nfiltered_results = cube.query(\"product_sales\")\n</code></pre>"},{"location":"concepts/relationships/","title":"Relationships in Cube Alchemy","text":"<p>Cube Alchemy uses implicit relationships, meaning DataFrames automatically connect to each other through shared column names.</p> <p>These relationships enable declarative work with newly connected data. The library handles the necessary data operations under the hood.</p> <p>At a high level, Cube Alchemy scans tables for shared columns, creates link/composite tables with auto keys when needed, and stores a relationship map plus join keys. Downstream operations (dimensions, metrics, queries) traverse this map to fetch only the columns required.</p>"},{"location":"concepts/relationships/#single-path","title":"Single Path","text":"<p>Cube Alchemy ensures there is exactly one bidirectional path between any two tables in the data model. This creates a clear, unambiguous way to traverse from one table to another.</p>"},{"location":"concepts/relationships/#composite-key","title":"Composite Key","text":"<p>When tables share multiple columns, Cube Alchemy automatically creates bridges to connect them:</p> <ol> <li> <p>Detection: It identifies sets of shared columns between tables</p> </li> <li> <p>Composite Key Creation: A single key is generated from these shared columns, autonumbered to improve performance on the upcoming operations.</p> </li> <li> <p>Composite Tables: New tables are created to store only the shared columns along with the generated keys</p> </li> <li> <p>Column Renaming: The original shared columns in source tables are renamed with a format of <code>column_name &lt;table_name&gt;</code></p> </li> <li> <p>Key Addition: The newly created composite keys are added to the original tables</p> </li> </ol> <p>This process consolidates complex relationships into simple, efficient connections while preserving all the original data.</p>"},{"location":"concepts/relationships/#cardinality","title":"Cardinality","text":"<p>Cube Alchemy takes a flexible approach:</p> <ul> <li> <p>It does not enforce specific cardinality constraints like one-to-many or many-to-one.</p> </li> <li> <p>Understanding the natural cardinality of the data helps avoid unexpected results in aggregations (e.g., due to row duplication).</p> </li> </ul> <p>Fact and Dimension Tables</p> <p>Data analysts are often familiar with dimensional modeling principles used in data warehousing and business intelligence. These typically involve:</p> <ul> <li> <p>Fact tables: Contain quantitative data that represent events or transactions (sales, orders, shipments).</p> </li> <li> <p>Dimension tables: Provide descriptive context through attributes that answer the \"who, what, where, when, why, and how\" questions about the data (customers, products, locations, time periods).</p> </li> </ul> <p>Note that the library is flexible regarding relationships between data rather than enforcing a specific modeling paradigm:</p> <ul> <li> <p>Cube Alchemy does not explicitly enforce Fact and Dimension table distinctions.</p> </li> <li> <p>Every column provided in the DataFrames used to define a Hypercube is available as a dimension and can also be used to create metrics and filters.</p> </li> <li> <p>The framework is design-agnostic. The analyst defines which tables serve as facts or dimensions in a given context.</p> </li> </ul> <pre><code># After initiating the hypercube, inspect how the connected tables relate\ncube.get_cardinalities()\n\n# Or inspect the relationships\ncube.get_relationship_matrix()  \n</code></pre>"},{"location":"concepts/relationships/#visualization","title":"Visualization","text":"<p>When setting up a hypercube, it can be useful to inspect the relationship graph to confirm how tables connect and where composite keys have been introduced.  <pre><code># Default view shows renamed columns (column_name &lt;table_name&gt;)\ncube.visualize_graph()\n\n# For cleaner display without table name suffixes\ncube.visualize_graph(full_column_names=False)\n</code></pre> This visualization helps illustrate the automatic transformations applied by Cube Alchemy.</p> <p>Note: If the displayed graph doesn't render well, try again or adjust the size.</p>"},{"location":"concepts/transformation/","title":"Transformation","text":"<p>Transformations are designed to add analytical features to a query DataFrame. Think of it as post-query augmentation: you define metrics and queries as usual, then apply transformers to compute things like moving averages, z-scores, Pareto flags, ranks, forecasts, or k-means clusters on the returned table.</p> <p>We apply a transformation to the DataFrame, which (ideally) enriches it by adding one or more new analytical features. After the transformation, the original DataFrame might be different from what it was before the transformation (e.g., it has a new column or a column was updated).</p>"},{"location":"concepts/transformation/#when-to-use-it","title":"When to use it","text":"<ul> <li> <p>You want quick analytical features derived from the queried data. Applying them through Derived Metrics is infeasible or difficult to implement.</p> </li> <li> <p>You want pluggable, composable transformations that can be reused across queries.</p> </li> </ul>"},{"location":"concepts/transformation/#interface-design","title":"Interface design","text":"<p>Designed for extensibility as a small registry-based interface:</p> <ul> <li> <p>Registry: register transformers by name using <code>register_transformer(name, transformer)</code>.</p> </li> <li> <p>Definition: attach a transformer to a query via <code>define_transformation(query_name, transformer, params)</code>.</p> </li> <li> <p>Execution: apply at runtime with <code>transform(df, transformer, **params)</code> or use stored configs to transform a query result.</p> </li> <li> <p>Configuration: store/retrieve per-query transformation configs with <code>list_transformations</code>, <code>get_transformation_config</code>, and <code>delete_transformation</code>.</p> </li> </ul> <p>A transformer can be either:</p> <ul> <li> <p>An object implementing <code>Transformer</code> class (ABC) with a method <code>transform(df: pd.DataFrame, **params) -&gt; pd.DataFrame</code>.</p> </li> <li> <p>A plain callable <code>fn(df: pd.DataFrame, **params) -&gt; pd.DataFrame</code>.</p> </li> </ul>"},{"location":"concepts/transformation/#default-transformers","title":"Default transformers","text":"<p>Cube Alchemy ships with a few defaults that are auto-registered:</p> <ul> <li> <p>moving_average</p> </li> <li> <p>cumulative</p> </li> <li> <p>rank</p> </li> <li> <p>zscore</p> </li> <li> <p>pareto</p> </li> <li> <p>linear_regression</p> </li> <li> <p>kmeans</p> </li> </ul> <p>You can inspect these under <code>cube_alchemy/transformation/default_transformers</code>. You can use them directly, or register your own custom functions.</p>"},{"location":"concepts/transformation/#examples","title":"Examples","text":"<p>Register a custom transformer and attach it to a query:</p> <pre><code>def add_simple_rank(df, by: str, ascending: bool = False, col: str = 'Simple Rank'):\n    df = df\n    df[col] = df[by].rank(ascending=ascending, method='dense').astype(int)\n    return df\n\ncube.register_transformer('rank', add_simple_rank)\ncube.define_transformation('Sales by Country and Category', transformer='rank', params={'by': 'Revenue'})\n\n# Later during execution, the newly generated column will be available\ncube.query('Sales by Country and Category')\n\n# This is a very simple case which can also be achieved using expressions\ncube.define_derived_metric(\n    name=\"Revenue Rank\",\n    expression='@pd.Series([Revenue]).rank(ascending=True, method=\"dense\")'\n)\n\n# But they operate in a different way and also on different steps on the query execution pipeline.\n\n# Derived Metrics create a new column on the DataFrame based on Dimensions and Metrics. Transformers transform the DataFrame using an arbitrary transformation function (it can delete rows, rename columns, add columns, change order, fetch external data through an api and merge it, etcetc.).\n</code></pre> <p>Notes:</p> <ul> <li> <p>Each query can configure at most one instance of the same transformer.</p> </li> <li> <p>When designing custom transformers, bear in mind that they may modify the query result, use with discretion.</p> </li> </ul>"},{"location":"concepts/workflow/","title":"Workflow","text":"<pre><code>flowchart LR\n  A[\"Load Data\"] --&gt; B[\"Build Hypercube\"]\n  subgraph MS[\"Model Specification\"]\n    C[\"Define Metrics\"] --&gt; D[\"Define Queries\"]\n  end\n  B --&gt; C\n  D --&gt; P[\"Define Plots\"]\n  P --&gt; E[\"Execute Queries\"]\n  E --&gt; F[\"Display\"]\n  F --&gt; G[\"Filter\"]\n  G --&gt; E</code></pre> <p>Cube Alchemy's workflow: load your data, build a unified hypercube, specify your model (metrics and queries), define plots, then execute. The stateful architecture lets you iterate by applying filters and re-running queries and plots.</p>"}]}